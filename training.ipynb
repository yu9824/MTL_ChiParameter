{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f319f559",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- Dataset, DataLoaderを使った形に書き換える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd403cc4-448f-47fd-9785-b281792e8abe",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47e5f2a-2b5b-498b-94db-6d13be73ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Any, Literal, TypeVar, Generic\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "if sys.version_info >= (3, 9):\n",
    "    from collections.abc import Sequence, Callable\n",
    "else:\n",
    "    from typing import Sequence, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.constants import zero_Celsius\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GroupKFold, KFold, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    max_error,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d431768",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449544b-fbe8-4db0-9e11-716d773f88e4",
   "metadata": {},
   "source": [
    "### User parameters\n",
    "\n",
    "recorded the actual values when training the model for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70039dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLNAME_SOLUBILITY_POLYMER_PAIR = \"ps_pair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ed9ffe-b879-4929-a03f-be0fe97ff8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "IDX_CV_TEST = 1\n",
    "\"\"\"index of the set of randomly split test data stored in test_cv_idx.pkl\"\"\"\n",
    "\n",
    "# data splitting\n",
    "test_ratio = 0.2\n",
    "\"\"\"ratio of total data in computational Chi data and solubility data used for test data\"\"\"\n",
    "n_CV_val = 5\n",
    "\"\"\"k-fold cross-validation for hyperparameter tuning\"\"\"\n",
    "test_ratio_final = 0.2\n",
    "\"\"\"ratio of training data used for validation during final training after selection of the best hyperparameters\"\"\"\n",
    "\n",
    "# hyperparameters\n",
    "n_hpara = 100\n",
    "\"\"\"number of samples in the hyperparameter space\"\"\"\n",
    "learning_rates = [0.001, 0.01]\n",
    "\"\"\"range for learning rates\"\"\"\n",
    "alpha1s = (0.0, 1.0)\n",
    "\"\"\"range for lambda_c (can be set to zero only for 2-task model excluding PoLyInfo dataset)\"\"\"\n",
    "alpha2s = (0.0, 1.0)\n",
    "\"\"\"range for lambda_s (can be set to zero only for 2-task model excluding COSMO-RS dataset)\"\"\"\n",
    "dim_outs = (3, 40)\n",
    "\"\"\"range for dimension of Z\"\"\"\n",
    "\n",
    "# model training\n",
    "dir_base = f\"hyper_groupCV/MT_testset_{IDX_CV_TEST}\"\n",
    "\"\"\"output directory for hyperparameter selection\"\"\"\n",
    "n_final_model = 10\n",
    "\"\"\"number of retrained model ensembles after selection of the best hyperparameters\"\"\"\n",
    "\n",
    "n_NNlayer = 3\n",
    "\"\"\"number of layer in the sub-network of fully connection MLPs\"\"\"\n",
    "sch_step_size = 10\n",
    "\"\"\"step size for learning rate scheduler\"\"\"\n",
    "sch_gamma = 0.5\n",
    "\"\"\"gamma parameter for learning rate scheduler\"\"\"\n",
    "epochs_s = 0\n",
    "\"\"\"number of pre-training steps based on solubility data (not used in the paper)\"\"\"\n",
    "epochs = 50\n",
    "\"\"\"number of max. epoch for the main training\"\"\"\n",
    "burn_in = 1\n",
    "\"\"\"burn in epoch (skip this number of epoch when selecting the epoch with the lowest validation loss)\"\"\"\n",
    "\n",
    "n_minibatch_PI = 20\n",
    "\"\"\"number of minibatch for solubility data\"\"\"\n",
    "n_minibatch_COSMO = 10\n",
    "\"\"\"number of minibatch for computational Chi data\"\"\"\n",
    "n_minibatch_CHI = 5\n",
    "\"\"\"number minibatch for experimental Chi data\"\"\"\n",
    "n_factor_CHI = int(n_minibatch_PI / n_minibatch_CHI)\n",
    "n_factor_COSMO = int(n_minibatch_PI / n_minibatch_COSMO)\n",
    "\n",
    "# other internal parameters\n",
    "temp_dim = 1\n",
    "\"\"\"take 1 or 2 only. 1: linear temperature dependence only; 2: 1/T^2 term included\"\"\"\n",
    "loss_factor_target = 1\n",
    "\"\"\"multiplier to adjust target loss contribution to total loss for model training\"\"\"\n",
    "no_target_BN = True\n",
    "\"\"\"True: do not include experimental Chi data for batch normalization in training\"\"\"\n",
    "no_COSMO_BN = True\n",
    "\"\"\"True: do not include COSMO data for batch normalization in training\"\"\"\n",
    "\n",
    "seed_hyper = 2022\n",
    "\"\"\"random seed for hyperparameter samples\"\"\"\n",
    "\n",
    "rng = np.random.default_rng(seed_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c505e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data container\n",
    "@dataclass\n",
    "class TargetContainer(Generic[T]):\n",
    "    PI: Optional[T] = None\n",
    "    COSMO: Optional[T] = None\n",
    "    Chi: Optional[T] = None\n",
    "\n",
    "    def keys(self) -> tuple[Literal[\"PI\", \"COSMO\", \"Chi\"], ...]:\n",
    "        return tuple(\n",
    "            _key\n",
    "            for _key in (\"PI\", \"COSMO\", \"Chi\")\n",
    "            if getattr(self, _key) is not None\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, _key: Literal[\"PI\", \"COSMO\", \"Chi\"]) -> T:\n",
    "        if _key in (\"PI\", \"COSMO\", \"Chi\"):\n",
    "            return getattr(self, _key)\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f74c96-2a19-4270-a513-666c3c34c782",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8e04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRPATH_LOAD = \"./sample_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba7fc5",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a4d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_pair</th>\n",
       "      <th>polymer_class</th>\n",
       "      <th>soluble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ps_pair polymer_class  soluble\n",
       "0  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   polychlorox     True\n",
       "1  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   polychlorox     True\n",
       "2  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   polychlorox     True\n",
       "3  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   polychlorox     True\n",
       "4  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   polychlorox     True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_container = TargetContainer(\n",
    "    PI=pd.read_csv(f\"{DIRPATH_LOAD}/data_PI.csv\", index_col=0)\n",
    ")\n",
    "print(data_container.PI.shape)\n",
    "data_container.PI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a66359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_pair</th>\n",
       "      <th>temp</th>\n",
       "      <th>chi</th>\n",
       "      <th>polymer_class</th>\n",
       "      <th>ps_pair_nonCyclic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.358063</td>\n",
       "      <td>cellulose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.358888</td>\n",
       "      <td>cellulose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.327410</td>\n",
       "      <td>cellulose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.604347</td>\n",
       "      <td>cellulose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.556457</td>\n",
       "      <td>cellulose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ps_pair   temp       chi  \\\n",
       "0  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...  100.0  0.358063   \n",
       "1  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...  100.0  0.358888   \n",
       "2  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...  100.0  0.327410   \n",
       "3  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...  100.0  0.604347   \n",
       "4  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...  100.0  0.556457   \n",
       "\n",
       "  polymer_class ps_pair_nonCyclic  \n",
       "0     cellulose               NaN  \n",
       "1     cellulose               NaN  \n",
       "2     cellulose               NaN  \n",
       "3     cellulose               NaN  \n",
       "4     cellulose               NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_container.COSMO = pd.read_csv(\n",
    "    f\"{DIRPATH_LOAD}/data_COSMO.csv\", index_col=0\n",
    ")\n",
    "print(data_container.COSMO.shape)\n",
    "data_container.COSMO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb0252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_pair</th>\n",
       "      <th>ps_pair_nonCyclic</th>\n",
       "      <th>temp</th>\n",
       "      <th>chi</th>\n",
       "      <th>polymer_class</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>SIGN2</th>\n",
       "      <th>SIGN3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>*C/C=C(\\Cl)C*_ClC(Cl)(Cl)Cl</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>*C/C=C(\\Cl)C*_[H]C(Cl)(Cl)Cl</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>*C/C=C(\\Cl)C*_[H]C(Cl)=C(Cl)Cl</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>*C/C=C(\\Cl)C*_[H]C([H])(Cl)C([H])([H])Cl</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...</td>\n",
       "      <td>*C/C=C(\\Cl)C*_[H]C([H])(Cl)Cl</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>polychlorox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ps_pair  \\\n",
       "0  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   \n",
       "1  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   \n",
       "2  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   \n",
       "3  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   \n",
       "4  [H]/C1=C(/Cl)C([H])([H])C([H])([H])/C([H])=C(\\...   \n",
       "\n",
       "                          ps_pair_nonCyclic   temp   chi polymer_class  DIFF  \\\n",
       "0               *C/C=C(\\Cl)C*_ClC(Cl)(Cl)Cl  100.0  0.23   polychlorox   NaN   \n",
       "1              *C/C=C(\\Cl)C*_[H]C(Cl)(Cl)Cl  100.0  0.28   polychlorox   NaN   \n",
       "2            *C/C=C(\\Cl)C*_[H]C(Cl)=C(Cl)Cl  100.0  0.24   polychlorox   NaN   \n",
       "3  *C/C=C(\\Cl)C*_[H]C([H])(Cl)C([H])([H])Cl  100.0  0.48   polychlorox   NaN   \n",
       "4             *C/C=C(\\Cl)C*_[H]C([H])(Cl)Cl  100.0  0.43   polychlorox   NaN   \n",
       "\n",
       "  SIGN2 SIGN3  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_container.Chi = pd.read_csv(f\"{DIRPATH_LOAD}/data_Chi.csv\", index_col=0)\n",
    "print(data_container.Chi.shape)\n",
    "data_container.Chi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034039f1",
   "metadata": {},
   "source": [
    "### Descriptor\n",
    "\n",
    "maybe calculate by `sample_code_for_descriptor_calculation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d3040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 794)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_mass_H</th>\n",
       "      <th>Polymer_mass_C</th>\n",
       "      <th>Polymer_mass_N</th>\n",
       "      <th>Polymer_mass_O</th>\n",
       "      <th>Polymer_mass_F</th>\n",
       "      <th>Polymer_mass_P</th>\n",
       "      <th>Polymer_mass_S</th>\n",
       "      <th>Polymer_mass_Cl</th>\n",
       "      <th>Polymer_mass_Br</th>\n",
       "      <th>Polymer_mass_I</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_fr_sulfide</th>\n",
       "      <th>Solvent_fr_sulfonamd</th>\n",
       "      <th>Solvent_fr_sulfone</th>\n",
       "      <th>Solvent_fr_term_acetylene</th>\n",
       "      <th>Solvent_fr_tetrazole</th>\n",
       "      <th>Solvent_fr_thiazole</th>\n",
       "      <th>Solvent_fr_thiocyan</th>\n",
       "      <th>Solvent_fr_thiophene</th>\n",
       "      <th>Solvent_fr_unbrch_alkane</th>\n",
       "      <th>Solvent_fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polymer_mass_H  Polymer_mass_C  Polymer_mass_N  Polymer_mass_O  \\\n",
       "0        0.586817        0.508618        0.439979        0.357449   \n",
       "1        0.586817        0.508618        0.439979        0.357449   \n",
       "2        0.586817        0.508618        0.439979        0.357449   \n",
       "3        0.586817        0.508618        0.439979        0.357449   \n",
       "4        0.586817        0.508618        0.439979        0.357449   \n",
       "\n",
       "   Polymer_mass_F  Polymer_mass_P  Polymer_mass_S  Polymer_mass_Cl  \\\n",
       "0        0.227742        0.081921        0.089032          0.10039   \n",
       "1        0.227742        0.081921        0.089032          0.10039   \n",
       "2        0.227742        0.081921        0.089032          0.10039   \n",
       "3        0.227742        0.081921        0.089032          0.10039   \n",
       "4        0.227742        0.081921        0.089032          0.10039   \n",
       "\n",
       "   Polymer_mass_Br  Polymer_mass_I  ...  Solvent_fr_sulfide  \\\n",
       "0     1.485486e-12    1.472831e-47  ...                   0   \n",
       "1     1.485486e-12    1.472831e-47  ...                   0   \n",
       "2     1.485486e-12    1.472831e-47  ...                   0   \n",
       "3     1.485486e-12    1.472831e-47  ...                   0   \n",
       "4     1.485486e-12    1.472831e-47  ...                   0   \n",
       "\n",
       "   Solvent_fr_sulfonamd  Solvent_fr_sulfone  Solvent_fr_term_acetylene  \\\n",
       "0                     0                   0                          0   \n",
       "1                     0                   0                          0   \n",
       "2                     0                   0                          0   \n",
       "3                     0                   0                          0   \n",
       "4                     0                   0                          0   \n",
       "\n",
       "   Solvent_fr_tetrazole  Solvent_fr_thiazole  Solvent_fr_thiocyan  \\\n",
       "0                     0                    0                    0   \n",
       "1                     0                    0                    0   \n",
       "2                     0                    0                    0   \n",
       "3                     0                    0                    0   \n",
       "4                     0                    0                    0   \n",
       "\n",
       "   Solvent_fr_thiophene  Solvent_fr_unbrch_alkane  Solvent_fr_urea  \n",
       "0                     0                         0                0  \n",
       "1                     0                         0                0  \n",
       "2                     0                         0                0  \n",
       "3                     0                         0                0  \n",
       "4                     0                         0                0  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_container = TargetContainer(\n",
    "    PI=pd.read_csv(f\"{DIRPATH_LOAD}/desc_PI.csv\", index_col=0)\n",
    ")\n",
    "print(desc_container.PI.shape)\n",
    "desc_container.PI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e177282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 794)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_mass_H</th>\n",
       "      <th>Polymer_mass_C</th>\n",
       "      <th>Polymer_mass_N</th>\n",
       "      <th>Polymer_mass_O</th>\n",
       "      <th>Polymer_mass_F</th>\n",
       "      <th>Polymer_mass_P</th>\n",
       "      <th>Polymer_mass_S</th>\n",
       "      <th>Polymer_mass_Cl</th>\n",
       "      <th>Polymer_mass_Br</th>\n",
       "      <th>Polymer_mass_I</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_fr_sulfide</th>\n",
       "      <th>Solvent_fr_sulfonamd</th>\n",
       "      <th>Solvent_fr_sulfone</th>\n",
       "      <th>Solvent_fr_term_acetylene</th>\n",
       "      <th>Solvent_fr_tetrazole</th>\n",
       "      <th>Solvent_fr_thiazole</th>\n",
       "      <th>Solvent_fr_thiocyan</th>\n",
       "      <th>Solvent_fr_thiophene</th>\n",
       "      <th>Solvent_fr_unbrch_alkane</th>\n",
       "      <th>Solvent_fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polymer_mass_H  Polymer_mass_C  Polymer_mass_N  Polymer_mass_O  \\\n",
       "0        0.586817        0.508618        0.439979        0.357449   \n",
       "1        0.586817        0.508618        0.439979        0.357449   \n",
       "2        0.586817        0.508618        0.439979        0.357449   \n",
       "3        0.586817        0.508618        0.439979        0.357449   \n",
       "4        0.586817        0.508618        0.439979        0.357449   \n",
       "\n",
       "   Polymer_mass_F  Polymer_mass_P  Polymer_mass_S  Polymer_mass_Cl  \\\n",
       "0        0.227742        0.081921        0.089032          0.10039   \n",
       "1        0.227742        0.081921        0.089032          0.10039   \n",
       "2        0.227742        0.081921        0.089032          0.10039   \n",
       "3        0.227742        0.081921        0.089032          0.10039   \n",
       "4        0.227742        0.081921        0.089032          0.10039   \n",
       "\n",
       "   Polymer_mass_Br  Polymer_mass_I  ...  Solvent_fr_sulfide  \\\n",
       "0     1.485486e-12    1.472831e-47  ...                   0   \n",
       "1     1.485486e-12    1.472831e-47  ...                   0   \n",
       "2     1.485486e-12    1.472831e-47  ...                   0   \n",
       "3     1.485486e-12    1.472831e-47  ...                   0   \n",
       "4     1.485486e-12    1.472831e-47  ...                   0   \n",
       "\n",
       "   Solvent_fr_sulfonamd  Solvent_fr_sulfone  Solvent_fr_term_acetylene  \\\n",
       "0                     0                   0                          0   \n",
       "1                     0                   0                          0   \n",
       "2                     0                   0                          0   \n",
       "3                     0                   0                          0   \n",
       "4                     0                   0                          0   \n",
       "\n",
       "   Solvent_fr_tetrazole  Solvent_fr_thiazole  Solvent_fr_thiocyan  \\\n",
       "0                     0                    0                    0   \n",
       "1                     0                    0                    0   \n",
       "2                     0                    0                    0   \n",
       "3                     0                    0                    0   \n",
       "4                     0                    0                    0   \n",
       "\n",
       "   Solvent_fr_thiophene  Solvent_fr_unbrch_alkane  Solvent_fr_urea  \n",
       "0                     0                         0                0  \n",
       "1                     0                         0                0  \n",
       "2                     0                         0                0  \n",
       "3                     0                         0                0  \n",
       "4                     0                         0                0  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_container.COSMO = pd.read_csv(\n",
    "    f\"{DIRPATH_LOAD}/desc_COSMO.csv\", index_col=0\n",
    ")\n",
    "print(desc_container.COSMO.shape)\n",
    "desc_container.COSMO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b810a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 794)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_mass_H</th>\n",
       "      <th>Polymer_mass_C</th>\n",
       "      <th>Polymer_mass_N</th>\n",
       "      <th>Polymer_mass_O</th>\n",
       "      <th>Polymer_mass_F</th>\n",
       "      <th>Polymer_mass_P</th>\n",
       "      <th>Polymer_mass_S</th>\n",
       "      <th>Polymer_mass_Cl</th>\n",
       "      <th>Polymer_mass_Br</th>\n",
       "      <th>Polymer_mass_I</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_fr_sulfide</th>\n",
       "      <th>Solvent_fr_sulfonamd</th>\n",
       "      <th>Solvent_fr_sulfone</th>\n",
       "      <th>Solvent_fr_term_acetylene</th>\n",
       "      <th>Solvent_fr_tetrazole</th>\n",
       "      <th>Solvent_fr_thiazole</th>\n",
       "      <th>Solvent_fr_thiocyan</th>\n",
       "      <th>Solvent_fr_thiophene</th>\n",
       "      <th>Solvent_fr_unbrch_alkane</th>\n",
       "      <th>Solvent_fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polymer_mass_H  Polymer_mass_C  Polymer_mass_N  Polymer_mass_O  \\\n",
       "0        0.586817        0.508618        0.439979        0.357449   \n",
       "1        0.586817        0.508618        0.439979        0.357449   \n",
       "2        0.586817        0.508618        0.439979        0.357449   \n",
       "3        0.586817        0.508618        0.439979        0.357449   \n",
       "4        0.586817        0.508618        0.439979        0.357449   \n",
       "\n",
       "   Polymer_mass_F  Polymer_mass_P  Polymer_mass_S  Polymer_mass_Cl  \\\n",
       "0        0.227742        0.081921        0.089032          0.10039   \n",
       "1        0.227742        0.081921        0.089032          0.10039   \n",
       "2        0.227742        0.081921        0.089032          0.10039   \n",
       "3        0.227742        0.081921        0.089032          0.10039   \n",
       "4        0.227742        0.081921        0.089032          0.10039   \n",
       "\n",
       "   Polymer_mass_Br  Polymer_mass_I  ...  Solvent_fr_sulfide  \\\n",
       "0     1.485486e-12    1.472831e-47  ...                   0   \n",
       "1     1.485486e-12    1.472831e-47  ...                   0   \n",
       "2     1.485486e-12    1.472831e-47  ...                   0   \n",
       "3     1.485486e-12    1.472831e-47  ...                   0   \n",
       "4     1.485486e-12    1.472831e-47  ...                   0   \n",
       "\n",
       "   Solvent_fr_sulfonamd  Solvent_fr_sulfone  Solvent_fr_term_acetylene  \\\n",
       "0                     0                   0                          0   \n",
       "1                     0                   0                          0   \n",
       "2                     0                   0                          0   \n",
       "3                     0                   0                          0   \n",
       "4                     0                   0                          0   \n",
       "\n",
       "   Solvent_fr_tetrazole  Solvent_fr_thiazole  Solvent_fr_thiocyan  \\\n",
       "0                     0                    0                    0   \n",
       "1                     0                    0                    0   \n",
       "2                     0                    0                    0   \n",
       "3                     0                    0                    0   \n",
       "4                     0                    0                    0   \n",
       "\n",
       "   Solvent_fr_thiophene  Solvent_fr_unbrch_alkane  Solvent_fr_urea  \n",
       "0                     0                         0                0  \n",
       "1                     0                         0                0  \n",
       "2                     0                         0                0  \n",
       "3                     0                         0                0  \n",
       "4                     0                         0                0  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_container.Chi = pd.read_csv(f\"{DIRPATH_LOAD}/desc_Chi.csv\", index_col=0)\n",
    "print(desc_container.Chi.shape)\n",
    "desc_container.Chi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85d123",
   "metadata": {},
   "source": [
    "### knowledge-based information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de23de4",
   "metadata": {},
   "source": [
    "- p: polymer\n",
    "- s: solvent\n",
    "\n",
    "\n",
    "- rd: rdkit descriptor\n",
    "- ff: radonpy force field parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b649499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DIRPATH_LOAD}/desc_names.json\", mode=\"r\") as f:\n",
    "    dict_desc_names: dict[\n",
    "        Literal[\"p_ff\", \"p_rd\", \"s_ff\", \"s_rd\"], list[str]\n",
    "    ] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e16bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DIRPATH_LOAD}/test_cv_idx.pkl\", mode=\"rb\") as f:\n",
    "    test_cv_idx: dict[Literal[\"train\", \"test\"], list[np.ndarray[Any, int]]] = (\n",
    "        pickle.load(f)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ea7ae-7cd7-4254-943f-41c74348f11e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d196cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index container\n",
    "@dataclass\n",
    "class IndexContainer:\n",
    "    train: np.ndarray[Any, int]\n",
    "    test: np.ndarray[Any, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b88e6",
   "metadata": {},
   "source": [
    "### Exp-Chi data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f4d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cv_idx[\"train\"][IDX_CV_TEST], test_cv_idx[\"test\"][IDX_CV_TEST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaad809",
   "metadata": {},
   "source": [
    "### COSMO data splitting\n",
    "\n",
    "exclude test ps_pair cases first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1ea6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cv_idx[\"train\"][IDX_CV_TEST].shape, test_cv_idx[\"test\"][IDX_CV_TEST].shape\n",
    "index_containers = TargetContainer(\n",
    "    Chi=IndexContainer(\n",
    "        train=test_cv_idx[\"train\"][IDX_CV_TEST],\n",
    "        test=test_cv_idx[\"test\"][IDX_CV_TEST],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af40e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "424\n"
     ]
    }
   ],
   "source": [
    "# XXX: same pair has diffrent chi parameters?\n",
    "print(data_container.COSMO[COLNAME_SOLUBILITY_POLYMER_PAIR].duplicated().sum())\n",
    "print(data_container.PI[COLNAME_SOLUBILITY_POLYMER_PAIR].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05185ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(761,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HACK: もしCOSMO等のデータに重複がなければこのやり方が一番エレガント。\n",
    "# データに重複があるので、サンプルコードよりデータが減りすぎてしまう。\n",
    "pd.concat(\n",
    "    (\n",
    "        data_container.COSMO[COLNAME_SOLUBILITY_POLYMER_PAIR],\n",
    "        data_container.Chi[COLNAME_SOLUBILITY_POLYMER_PAIR],\n",
    "    ),\n",
    "    axis=0,\n",
    ").reset_index(drop=True).drop_duplicates(keep=\"first\").loc[\n",
    "    : data_container.COSMO.shape[0]\n",
    "    - 1  # `loc` includes end\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e51c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiのテストデータに含まれていないものの中から`test_ratio`分のテストデータを取り出す。\n",
    "# それ以外はすべて学習データに加える\n",
    "_flag_COSMO_in_Chi = data_container.COSMO[\n",
    "    COLNAME_SOLUBILITY_POLYMER_PAIR\n",
    "].isin(\n",
    "    data_container.Chi.loc[\n",
    "        test_cv_idx[\"test\"][IDX_CV_TEST],\n",
    "        COLNAME_SOLUBILITY_POLYMER_PAIR,\n",
    "    ]\n",
    ")\n",
    "_flag_COSMO_test = _flag_COSMO_in_Chi.copy()\n",
    "_flag_COSMO_test.loc[\n",
    "    rng.choice(\n",
    "        _flag_COSMO_in_Chi.index[~_flag_COSMO_in_Chi],\n",
    "        size=math.ceil(np.sum(~_flag_COSMO_in_Chi, axis=None) * test_ratio),\n",
    "        replace=False,\n",
    "    )\n",
    "] = True\n",
    "index_containers.COSMO = IndexContainer(\n",
    "    train=data_container.COSMO.index[~_flag_COSMO_test].to_numpy(),\n",
    "    test=data_container.COSMO.index[_flag_COSMO_test].to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ad46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiのテストデータに含まれていないものの中から`test_ratio`分のテストデータを取り出す。\n",
    "# それ以外はすべて学習データに加える\n",
    "_flag_PI_in_Chi = data_container.PI[COLNAME_SOLUBILITY_POLYMER_PAIR].isin(\n",
    "    data_container.Chi.loc[\n",
    "        test_cv_idx[\"test\"][IDX_CV_TEST],\n",
    "        COLNAME_SOLUBILITY_POLYMER_PAIR,\n",
    "    ]\n",
    ")\n",
    "_flag_PI_test = _flag_PI_in_Chi.copy()\n",
    "_flag_PI_test.loc[\n",
    "    rng.choice(\n",
    "        _flag_PI_in_Chi.index[~_flag_PI_in_Chi],\n",
    "        size=math.ceil(np.sum(~_flag_PI_in_Chi, axis=None) * test_ratio),\n",
    "        replace=False,\n",
    "    )\n",
    "] = True\n",
    "index_containers.PI = IndexContainer(\n",
    "    train=data_container.PI.index[~_flag_PI_test],\n",
    "    test=data_container.PI.index[_flag_PI_test],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49341865-bac2-45ad-956c-9d3ef9fef4c3",
   "metadata": {},
   "source": [
    "## Process data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a60e0-c116-4db4-8e30-026507df45a3",
   "metadata": {},
   "source": [
    "### Scale descriptors\n",
    "\n",
    "異なるデータセットであったとしても同じ特徴量であれば同じスケーラーで標準化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0335147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_MaxEStateIndex</th>\n",
       "      <th>Polymer_MinEStateIndex</th>\n",
       "      <th>Polymer_MaxAbsEStateIndex</th>\n",
       "      <th>Polymer_MinAbsEStateIndex</th>\n",
       "      <th>Polymer_qed</th>\n",
       "      <th>Polymer_MolWt</th>\n",
       "      <th>Polymer_HeavyAtomMolWt</th>\n",
       "      <th>Polymer_ExactMolWt</th>\n",
       "      <th>Polymer_NumValenceElectrons</th>\n",
       "      <th>Polymer_NumRadicalElectrons</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_fr_sulfide</th>\n",
       "      <th>Solvent_fr_sulfonamd</th>\n",
       "      <th>Solvent_fr_sulfone</th>\n",
       "      <th>Solvent_fr_term_acetylene</th>\n",
       "      <th>Solvent_fr_tetrazole</th>\n",
       "      <th>Solvent_fr_thiazole</th>\n",
       "      <th>Solvent_fr_thiocyan</th>\n",
       "      <th>Solvent_fr_thiophene</th>\n",
       "      <th>Solvent_fr_unbrch_alkane</th>\n",
       "      <th>Solvent_fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>885.37</td>\n",
       "      <td>834.97</td>\n",
       "      <td>880.079778</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>885.37</td>\n",
       "      <td>834.97</td>\n",
       "      <td>880.079778</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>885.37</td>\n",
       "      <td>834.97</td>\n",
       "      <td>880.079778</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>885.37</td>\n",
       "      <td>834.97</td>\n",
       "      <td>880.079778</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>6.426039</td>\n",
       "      <td>0.737513</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>885.37</td>\n",
       "      <td>834.97</td>\n",
       "      <td>880.079778</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polymer_MaxEStateIndex  Polymer_MinEStateIndex  Polymer_MaxAbsEStateIndex  \\\n",
       "2                6.426039                0.737513                   6.426039   \n",
       "3                6.426039                0.737513                   6.426039   \n",
       "4                6.426039                0.737513                   6.426039   \n",
       "5                6.426039                0.737513                   6.426039   \n",
       "7                6.426039                0.737513                   6.426039   \n",
       "\n",
       "   Polymer_MinAbsEStateIndex  Polymer_qed  Polymer_MolWt  \\\n",
       "2                   0.737513     0.227374         885.37   \n",
       "3                   0.737513     0.227374         885.37   \n",
       "4                   0.737513     0.227374         885.37   \n",
       "5                   0.737513     0.227374         885.37   \n",
       "7                   0.737513     0.227374         885.37   \n",
       "\n",
       "   Polymer_HeavyAtomMolWt  Polymer_ExactMolWt  Polymer_NumValenceElectrons  \\\n",
       "2                  834.97          880.079778                          280   \n",
       "3                  834.97          880.079778                          280   \n",
       "4                  834.97          880.079778                          280   \n",
       "5                  834.97          880.079778                          280   \n",
       "7                  834.97          880.079778                          280   \n",
       "\n",
       "   Polymer_NumRadicalElectrons  ...  Solvent_fr_sulfide  Solvent_fr_sulfonamd  \\\n",
       "2                            0  ...                   0                     0   \n",
       "3                            0  ...                   0                     0   \n",
       "4                            0  ...                   0                     0   \n",
       "5                            0  ...                   0                     0   \n",
       "7                            0  ...                   0                     0   \n",
       "\n",
       "   Solvent_fr_sulfone  Solvent_fr_term_acetylene  Solvent_fr_tetrazole  \\\n",
       "2                   0                          0                     0   \n",
       "3                   0                          0                     0   \n",
       "4                   0                          0                     0   \n",
       "5                   0                          0                     0   \n",
       "7                   0                          0                     0   \n",
       "\n",
       "   Solvent_fr_thiazole  Solvent_fr_thiocyan  Solvent_fr_thiophene  \\\n",
       "2                    0                    0                     0   \n",
       "3                    0                    0                     0   \n",
       "4                    0                    0                     0   \n",
       "5                    0                    0                     0   \n",
       "7                    0                    0                     0   \n",
       "\n",
       "   Solvent_fr_unbrch_alkane  Solvent_fr_urea  \n",
       "2                         0                0  \n",
       "3                         0                0  \n",
       "4                         0                0  \n",
       "5                         0                0  \n",
       "7                         1                0  \n",
       "\n",
       "[5 rows x 414 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_names_rdkit = dict_desc_names[\"p_rd\"] + dict_desc_names[\"s_rd\"]\n",
    "desc_rdkit_all = pd.concat(\n",
    "    (\n",
    "        desc_container.PI.loc[index_containers.PI.train, desc_names_rdkit],\n",
    "        desc_container.COSMO.loc[\n",
    "            index_containers.COSMO.train, desc_names_rdkit\n",
    "        ],\n",
    "        desc_container.Chi.loc[index_containers.Chi.train, desc_names_rdkit],\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "desc_rdkit_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5bc6caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;variancethreshold&#x27;, VarianceThreshold()),\n",
       "                (&#x27;powertransformer&#x27;, PowerTransformer())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;variancethreshold&#x27;, VarianceThreshold()),\n",
       "                (&#x27;powertransformer&#x27;, PowerTransformer())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;VarianceThreshold<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_selection.VarianceThreshold.html\">?<span>Documentation for VarianceThreshold</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>VarianceThreshold()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PowerTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.PowerTransformer.html\">?<span>Documentation for PowerTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PowerTransformer()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('variancethreshold', VarianceThreshold()),\n",
       "                ('powertransformer', PowerTransformer())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_X_rdkit_all = make_pipeline(\n",
    "    VarianceThreshold(), PowerTransformer(method=\"yeo-johnson\")\n",
    ").fit(desc_rdkit_all)\n",
    "scaler_X_rdkit_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2280481-3e63-465a-b3f2-eb27479bf6ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "Filter out constant descriptors (based on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0a845f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_mass_H</th>\n",
       "      <th>Polymer_mass_C</th>\n",
       "      <th>Polymer_mass_N</th>\n",
       "      <th>Polymer_mass_O</th>\n",
       "      <th>Polymer_mass_F</th>\n",
       "      <th>Polymer_mass_P</th>\n",
       "      <th>Polymer_mass_S</th>\n",
       "      <th>Polymer_mass_Cl</th>\n",
       "      <th>Polymer_mass_Br</th>\n",
       "      <th>Polymer_mass_I</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_k_dih_10</th>\n",
       "      <th>Solvent_k_dih_11</th>\n",
       "      <th>Solvent_k_dih_12</th>\n",
       "      <th>Solvent_k_dih_13</th>\n",
       "      <th>Solvent_k_dih_14</th>\n",
       "      <th>Solvent_k_dih_15</th>\n",
       "      <th>Solvent_k_dih_16</th>\n",
       "      <th>Solvent_k_dih_17</th>\n",
       "      <th>Solvent_k_dih_18</th>\n",
       "      <th>Solvent_k_dih_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>2.145700e-01</td>\n",
       "      <td>9.703123e-01</td>\n",
       "      <td>8.036671e-02</td>\n",
       "      <td>1.219166e-04</td>\n",
       "      <td>3.387440e-09</td>\n",
       "      <td>1.723862e-15</td>\n",
       "      <td>1.606776e-23</td>\n",
       "      <td>2.743029e-33</td>\n",
       "      <td>8.576845e-45</td>\n",
       "      <td>4.911868e-58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>1.279998e-70</td>\n",
       "      <td>5.784715e-87</td>\n",
       "      <td>4.788249e-105</td>\n",
       "      <td>7.259280e-125</td>\n",
       "      <td>2.015731e-146</td>\n",
       "      <td>1.025164e-169</td>\n",
       "      <td>9.549409e-195</td>\n",
       "      <td>1.629227e-221</td>\n",
       "      <td>5.091067e-250</td>\n",
       "      <td>2.913789e-280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>5.825000e-73</td>\n",
       "      <td>1.327453e-89</td>\n",
       "      <td>5.541533e-108</td>\n",
       "      <td>4.237384e-128</td>\n",
       "      <td>5.934810e-150</td>\n",
       "      <td>1.522468e-173</td>\n",
       "      <td>7.153478e-199</td>\n",
       "      <td>6.156180e-226</td>\n",
       "      <td>9.703515e-255</td>\n",
       "      <td>2.801366e-285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.357449</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.089032</td>\n",
       "      <td>0.10039</td>\n",
       "      <td>1.485486e-12</td>\n",
       "      <td>1.472831e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534702e-24</td>\n",
       "      <td>3.374939e-34</td>\n",
       "      <td>1.359345e-45</td>\n",
       "      <td>1.002803e-58</td>\n",
       "      <td>1.354951e-73</td>\n",
       "      <td>3.353156e-90</td>\n",
       "      <td>1.519868e-108</td>\n",
       "      <td>1.261770e-128</td>\n",
       "      <td>1.918565e-150</td>\n",
       "      <td>5.343119e-174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 380 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polymer_mass_H  Polymer_mass_C  Polymer_mass_N  Polymer_mass_O  \\\n",
       "2        0.586817        0.508618        0.439979        0.357449   \n",
       "3        0.586817        0.508618        0.439979        0.357449   \n",
       "4        0.586817        0.508618        0.439979        0.357449   \n",
       "5        0.586817        0.508618        0.439979        0.357449   \n",
       "7        0.586817        0.508618        0.439979        0.357449   \n",
       "\n",
       "   Polymer_mass_F  Polymer_mass_P  Polymer_mass_S  Polymer_mass_Cl  \\\n",
       "2        0.227742        0.081921        0.089032          0.10039   \n",
       "3        0.227742        0.081921        0.089032          0.10039   \n",
       "4        0.227742        0.081921        0.089032          0.10039   \n",
       "5        0.227742        0.081921        0.089032          0.10039   \n",
       "7        0.227742        0.081921        0.089032          0.10039   \n",
       "\n",
       "   Polymer_mass_Br  Polymer_mass_I  ...  Solvent_k_dih_10  Solvent_k_dih_11  \\\n",
       "2     1.485486e-12    1.472831e-47  ...      2.145700e-01      9.703123e-01   \n",
       "3     1.485486e-12    1.472831e-47  ...      1.279998e-70      5.784715e-87   \n",
       "4     1.485486e-12    1.472831e-47  ...      0.000000e+00      0.000000e+00   \n",
       "5     1.485486e-12    1.472831e-47  ...      5.825000e-73      1.327453e-89   \n",
       "7     1.485486e-12    1.472831e-47  ...      1.534702e-24      3.374939e-34   \n",
       "\n",
       "   Solvent_k_dih_12  Solvent_k_dih_13  Solvent_k_dih_14  Solvent_k_dih_15  \\\n",
       "2      8.036671e-02      1.219166e-04      3.387440e-09      1.723862e-15   \n",
       "3     4.788249e-105     7.259280e-125     2.015731e-146     1.025164e-169   \n",
       "4      0.000000e+00      0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "5     5.541533e-108     4.237384e-128     5.934810e-150     1.522468e-173   \n",
       "7      1.359345e-45      1.002803e-58      1.354951e-73      3.353156e-90   \n",
       "\n",
       "   Solvent_k_dih_16  Solvent_k_dih_17  Solvent_k_dih_18  Solvent_k_dih_19  \n",
       "2      1.606776e-23      2.743029e-33      8.576845e-45      4.911868e-58  \n",
       "3     9.549409e-195     1.629227e-221     5.091067e-250     2.913789e-280  \n",
       "4      0.000000e+00      0.000000e+00      0.000000e+00      0.000000e+00  \n",
       "5     7.153478e-199     6.156180e-226     9.703515e-255     2.801366e-285  \n",
       "7     1.519868e-108     1.261770e-128     1.918565e-150     5.343119e-174  \n",
       "\n",
       "[5 rows x 380 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_names_forcefield = dict_desc_names[\"p_ff\"] + dict_desc_names[\"s_ff\"]\n",
    "desc_forcefield_all = pd.concat(\n",
    "    (\n",
    "        desc_container.PI.loc[\n",
    "            index_containers.PI.train, desc_names_forcefield\n",
    "        ],\n",
    "        desc_container.COSMO.loc[\n",
    "            index_containers.COSMO.train, desc_names_forcefield\n",
    "        ],\n",
    "        desc_container.Chi.loc[\n",
    "            index_containers.Chi.train, desc_names_forcefield\n",
    "        ],\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "desc_forcefield_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b38dab4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VarianceThreshold()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;VarianceThreshold<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_selection.VarianceThreshold.html\">?<span>Documentation for VarianceThreshold</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>VarianceThreshold()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "VarianceThreshold()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_X_ff_all = VarianceThreshold().fit(desc_forcefield_all)\n",
    "selector_X_ff_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d39bf-2df9-46c8-aa69-065b8619aada",
   "metadata": {},
   "source": [
    "Setup and scale y values and temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424e3e8",
   "metadata": {},
   "source": [
    "s: source, t: target\n",
    "\n",
    "- `y_s0`: `y_container.PI`\n",
    "- `y_s`: `y_container.COSMO`\n",
    "- `y_t`: `y_container.Chi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9fe062",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_container = TargetContainer(\n",
    "    PI=data_container.PI[[\"soluble\"]],\n",
    "    COSMO=data_container.COSMO[[\"chi\"]],\n",
    "    Chi=data_container.Chi[[\"chi\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fca12b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        T1\n",
       "0  0.00268\n",
       "1  0.00268\n",
       "2  0.00268\n",
       "3  0.00268\n",
       "4  0.00268"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_temperature(\n",
    "    sr: pd.Series, temp_dim: Literal[1, 2]\n",
    ") -> pd.DataFrame:\n",
    "    return pd.concat(\n",
    "        (\n",
    "            (1 / ((sr + zero_Celsius) ** _dim)).rename(f\"T{_dim}\")\n",
    "            for _dim in range(1, temp_dim + 1)\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "temperature_container = TargetContainer(\n",
    "    COSMO=convert_temperature(data_container.COSMO[\"temp\"], temp_dim=temp_dim),\n",
    "    Chi=convert_temperature(data_container.Chi[\"temp\"], temp_dim=temp_dim),\n",
    ")\n",
    "temperature_container.COSMO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d19b36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y_container = TargetContainer(\n",
    "    COSMO=StandardScaler(), Chi=StandardScaler()\n",
    ")\n",
    "\n",
    "y_scaled_container = TargetContainer(PI=y_container.PI)\n",
    "for _key in scaler_y_container.keys():\n",
    "    if isinstance(getattr(scaler_y_container, _key), StandardScaler):\n",
    "        _scaler_y: StandardScaler = getattr(scaler_y_container, _key)\n",
    "        _y: pd.DataFrame = getattr(y_container, _key)\n",
    "        _index_container: IndexContainer = getattr(index_containers, _key)\n",
    "        _scaler_y.fit(_y.loc[_index_container.train])\n",
    "        setattr(\n",
    "            y_scaled_container,\n",
    "            _key,\n",
    "            pd.DataFrame(\n",
    "                _scaler_y.transform(_y), index=_y.index, columns=_y.columns\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "551209dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_scaled_container: TargetContainer[pd.DataFrame] = TargetContainer()\n",
    "\n",
    "scaler_temperature_container = TargetContainer(\n",
    "    COSMO=StandardScaler(), Chi=StandardScaler()\n",
    ")\n",
    "for _key in scaler_temperature_container.keys():\n",
    "    _scaler_temperature: StandardScaler = getattr(\n",
    "        scaler_temperature_container, _key\n",
    "    )\n",
    "    _df_temperature: pd.DataFrame = getattr(temperature_container, _key)\n",
    "    _index_container: IndexContainer = getattr(index_containers, _key)\n",
    "\n",
    "    _scaler_temperature.fit(_df_temperature.loc[_index_container.train])\n",
    "    setattr(\n",
    "        temperature_scaled_container,\n",
    "        _key,\n",
    "        pd.DataFrame(\n",
    "            _scaler_temperature.transform(_df_temperature),\n",
    "            index=_df_temperature.index,\n",
    "            columns=_df_temperature.columns,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d5747-812f-4c56-8e65-9c1b8a0a1731",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc11b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed linearly reducing pyramid shape\n",
    "def neuron_vector(nL: int, in_neu: int, out_neu: int) -> tuple[int]:\n",
    "    return tuple(\n",
    "        int(x) for x in np.rint(np.linspace(in_neu, out_neu, nL + 2))[1:-1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ce32d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.,  2.],\n",
       "        [ 0.,  3.,  4.,  5.],\n",
       "        [ 0.,  6.,  7.,  8.],\n",
       "        [ 0.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    (torch.zeros((4, 1)), torch.tensor(np.arange(12).reshape(4, 3))), dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf134e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChiModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_polymer: nn.Module,\n",
    "        model_solvent: nn.Module,\n",
    "        dim_ur: int,  # おそらく、uとrの区切れがどこにあるかを表すパラメータ\n",
    "        temp_dim: Literal[1, 2],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_polymer = deepcopy(model_polymer)\n",
    "        self.model_solvent = deepcopy(model_solvent)\n",
    "        self.dim_ur = dim_ur\n",
    "        self.temp_dim = temp_dim\n",
    "\n",
    "        self.fc = nn.Linear(self.dim_ur, 3 + 2 * self.temp_dim)\n",
    "        \"\"\"fully connected layer\"\"\"\n",
    "        self.activation_function = nn.Sigmoid()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_polymer: torch.Tensor,\n",
    "        x_solvent: torch.Tensor,\n",
    "        x_temp: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        assert x_temp.shape[1] == self.temp_dim\n",
    "\n",
    "        # feature embedding\n",
    "        ur_polymer = self.model_polymer(x_polymer)\n",
    "        ur_solvent = self.model_solvent(x_solvent)\n",
    "\n",
    "        # > The first term calculates the elementwise squared difference of the k-dimensional latent vectors,\n",
    "        # u_p and u_s.\n",
    "        sp = torch.square(\n",
    "            ur_polymer[:, : self.dim_ur] - ur_solvent[:, : self.dim_ur]\n",
    "        )\n",
    "\n",
    "        # > The second and third terms denote the polymer- and solvent-specific discount factors, respectively,\n",
    "        # which are analogous to the interaction radius of the HSP sphere.\n",
    "        r_polymer = torch.square(ur_polymer[:, self.dim_ur :])\n",
    "        r_solvent = torch.square(ur_solvent[:, self.dim_ur :])\n",
    "\n",
    "        # equation 4 in paper; distance operator\n",
    "        z_raw = sp - r_polymer - r_solvent\n",
    "        z = self.fc(z_raw)\n",
    "\n",
    "        z_soluble = self.activation_function(z[:, [0]])\n",
    "\n",
    "        # おそらく、温度を線型外挿 (temp_dimが2の場合は2次の項もいれて) しようとしている。\n",
    "        z_cosmo = torch.sum(\n",
    "            torch.cat((torch.ones((x_temp.shape[0], 1)), x_temp), dim=1)\n",
    "            * z[1 : 2 + self.temp_dim],\n",
    "            dim=1,\n",
    "        )\n",
    "        z_chi = torch.sum(\n",
    "            torch.cat((torch.ones((x_temp.shape[0], 1)), x_temp), dim=1)\n",
    "            * z[2 + self.temp_dim : 3 + 2 * self.temp_dim],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return torch.cat((z_soluble, z_cosmo, z_chi, z, z_raw), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94be9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Base NN layer. This is a wrap around PyTorch.\n",
    "    See here for details: http://pytorch.org/docs/master/nn.html#\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        activation_func: Callable = nn.ReLU(),\n",
    "        normalizer: Optional[float] = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features:\n",
    "            Size of each input sample.\n",
    "        out_features:\n",
    "            Size of each output sample\n",
    "        bias:\n",
    "            If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        dropout: float\n",
    "            Probability of an element to be zeroed. Default: 0.5\n",
    "        activation_func: func\n",
    "            Activation function.\n",
    "        normalizer: func\n",
    "            Normalization layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalizer = (\n",
    "            None\n",
    "            if not normalizer\n",
    "            else nn.BatchNorm1d(out_features, momentum=normalizer)\n",
    "        )\n",
    "        self.activation = None if not activation_func else activation_func\n",
    "\n",
    "    def forward(self, x):\n",
    "        _out = self.linear(x)\n",
    "        if self.dropout:\n",
    "            _out = self.dropout(_out)\n",
    "        if self.normalizer:\n",
    "            _out = self.normalizer(_out)\n",
    "        if self.activation:\n",
    "            _out = self.activation(_out)\n",
    "        return _out\n",
    "\n",
    "\n",
    "class SequentialLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential model with linear layers and configurable other hype-parameters.\n",
    "    e.g. ``dropout``, ``hidden layers``\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        *,\n",
    "        h_neurons: Union[Sequence[float], Sequence[int]] = (),\n",
    "        h_bias: Union[bool, Sequence[bool]] = True,\n",
    "        h_dropouts: Union[float, Sequence[float]] = 0.0,\n",
    "        h_normalizers: Union[float, None, Sequence[Optional[float]]] = 0.1,\n",
    "        h_activation_funcs: Union[\n",
    "            Callable, None, Sequence[Optional[Callable]]\n",
    "        ] = nn.ReLU(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features\n",
    "            Size of input.\n",
    "        out_features\n",
    "            Size of output.\n",
    "        bias\n",
    "            Enable ``bias`` in input layer.\n",
    "        h_neurons\n",
    "            Number of neurons in hidden layers.\n",
    "            Can be a tuple of floats. In that case,\n",
    "            all these numbers will be used to calculate the neuron numbers.\n",
    "            e.g. (0.5, 0.4, ...) will be expanded as (in_features * 0.5, in_features * 0.4, ...)\n",
    "        h_bias\n",
    "            ``bias`` in hidden layers.\n",
    "        h_dropouts\n",
    "            Probabilities of dropout in hidden layers.\n",
    "        h_normalizers\n",
    "            Momentum of batched normalizers in hidden layers.\n",
    "        h_activation_funcs\n",
    "            Activation functions in hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._h_layers = len(h_neurons)\n",
    "        if self._h_layers > 0:\n",
    "            if isinstance(h_neurons[0], float):\n",
    "                tmp = [in_features]\n",
    "                for i, ratio in enumerate(h_neurons):\n",
    "                    num = math.ceil(in_features * ratio)\n",
    "                    tmp.append(num)\n",
    "                neurons = tuple(tmp)\n",
    "\n",
    "            elif isinstance(h_neurons[0], int):\n",
    "                neurons = (in_features,) + tuple(h_neurons)\n",
    "            else:\n",
    "                raise RuntimeError(\"illegal parameter type of <h_neurons>\")\n",
    "\n",
    "            activation_funcs = self._check_input(h_activation_funcs)\n",
    "            normalizers = self._check_input(h_normalizers)\n",
    "            dropouts = self._check_input(h_dropouts)\n",
    "            bias = (bias,) + self._check_input(h_bias)\n",
    "\n",
    "            for i in range(self._h_layers):\n",
    "                setattr(\n",
    "                    self,\n",
    "                    f\"layer_{i}\",\n",
    "                    LinearLayer(\n",
    "                        in_features=neurons[i],\n",
    "                        out_features=neurons[i + 1],\n",
    "                        bias=bias[i],\n",
    "                        dropout=dropouts[i],\n",
    "                        activation_func=activation_funcs[i],\n",
    "                        normalizer=normalizers[i],\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            self.output = nn.Linear(neurons[-1], out_features, bias[-1])\n",
    "        else:\n",
    "            self.output = nn.Linear(in_features, out_features, bias)\n",
    "\n",
    "    def _check_input(self, i):\n",
    "        if isinstance(i, Sequence):\n",
    "            if len(i) != self._h_layers:\n",
    "                raise RuntimeError(\n",
    "                    f\"number of parameter not consistent with number of layers, \"\n",
    "                    f\"input is {len(i)} but need to be {self._h_layers}\"\n",
    "                )\n",
    "            return tuple(i)\n",
    "        else:\n",
    "            return tuple([i] * self._h_layers)\n",
    "\n",
    "    def forward(self, x: Any) -> Any:\n",
    "        for i in range(self._h_layers):\n",
    "            x = getattr(self, f\"layer_{i}\")(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a38ec1e-2972-4d1f-8f6c-7d7f16e2ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to save and load the NN model\n",
    "def save_NN(\n",
    "    paras_p: dict,\n",
    "    paras_s: dict,\n",
    "    dim_out: int,\n",
    "    c_mdl: nn.Module,\n",
    "    file_name: str,\n",
    "):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_p\": paras_p,\n",
    "            \"model_s\": paras_s,\n",
    "            \"chi\": c_mdl.state_dict(),\n",
    "            \"dim_out\": dim_out,\n",
    "        },\n",
    "        file_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_NN(file_name: str):\n",
    "    tmp_paras = torch.load(file_name)\n",
    "    c_model = ChiModel(\n",
    "        SequentialLinear(**tmp_paras[\"model_p\"]),\n",
    "        SequentialLinear(**tmp_paras[\"model_s\"]),\n",
    "        tmp_paras[\"dim_out\"],\n",
    "    )\n",
    "    _ = c_model.load_state_dict(tmp_paras[\"chi\"])\n",
    "    return c_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33695b14-7fab-4925-99bc-3e37a9286eea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter tuning with grid search and CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dede587",
   "metadata": {},
   "source": [
    "まずはCOSMOとPIのデータで学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b3e6-967b-4484-86e9-6cede2b680af",
   "metadata": {},
   "source": [
    "Group validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bec09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_polymer: list[str] = data_container.Chi.loc[\n",
    "    index_containers.Chi.train, COLNAME_SOLUBILITY_POLYMER_PAIR\n",
    "].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d07c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kf = GroupKFold(n_splits=n_CV_val)\n",
    "\n",
    "index_containers_hyperparams_tuning: list[IndexContainer] = list()\n",
    "_y_chi_train: pd.DataFrame = y_container.Chi.loc[index_containers.Chi.train]\n",
    "for _index_train, _index_val in group_kf.split(\n",
    "    _y_chi_train, groups=groups_polymer\n",
    "):\n",
    "    index_containers_hyperparams_tuning.append(\n",
    "        IndexContainer(\n",
    "            train=_y_chi_train.iloc[_index_train].index.to_numpy(),\n",
    "            test=_y_chi_train.iloc[_index_val].index.to_numpy(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480abc3-ab1a-4164-9e3c-b705e7d412d5",
   "metadata": {},
   "source": [
    "Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df099525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 特徴量の取り出し方が違うので注意。\n",
    "# desc_names_polymer: tuple[str] = tuple(\n",
    "#     _desc_name\n",
    "#     for _desc_name in desc_names_forcefield\n",
    "#     if \"Polymer\" in _desc_name\n",
    "# ) + tuple(\n",
    "#     _desc_name for _desc_name in desc_names_rdkit if \"Polymer\" in _desc_name\n",
    "# )\n",
    "# desc_names_solvent: tuple[str] = tuple(\n",
    "#     _desc_name\n",
    "#     for _desc_name in desc_names_forcefield\n",
    "#     if \"Solvent\" in _desc_name\n",
    "# ) + tuple(\n",
    "#     _desc_name for _desc_name in desc_names_rdkit if \"Solvent\" in _desc_name\n",
    "# )\n",
    "# desc_names_polymer[:5], desc_names_solvent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a82d2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert desc_container.PI.shape[1] == len(desc_names_forcefield) + len(\n",
    "    desc_names_rdkit\n",
    ")\n",
    "desc_names_use = (\n",
    "    np.hstack(\n",
    "        (\n",
    "            selector_X_ff_all.get_feature_names_out(),\n",
    "            scaler_X_rdkit_all.get_feature_names_out(),\n",
    "        )\n",
    "    )\n",
    "    .astype(str)\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac4c497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetLikeObject:\n",
    "    x_polymer: torch.Tensor\n",
    "    x_solvent: torch.Tensor\n",
    "    x_temp: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetContainer:\n",
    "    train: DatasetLikeObject\n",
    "    test: DatasetLikeObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "048b216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0434,  0.2008,  0.1926,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2434,  0.2440,  0.2157,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2390,  0.3700,  0.3383,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        ...,\n",
      "        [ 0.6441,  0.5729,  0.4969,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.5640,  0.6397,  0.5771,  ..., -0.0568, 13.2853, -0.4971],\n",
      "        [ 0.5640,  0.6397,  0.5771,  ..., -0.0568, 13.2853, -0.4971]])\n",
      "tensor([[ 0.0434,  0.2008,  0.1926,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2434,  0.2440,  0.2157,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2390,  0.3700,  0.3383,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        ...,\n",
      "        [ 0.6046,  0.3913,  0.3246,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.7302,  0.4679,  0.3961,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.6710,  0.5120,  0.4516,  ..., -0.0568, -0.0753, -0.4971]])\n",
      "tensor([[ 0.0434,  0.2008,  0.1926,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2434,  0.2440,  0.2157,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.2390,  0.3700,  0.3383,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        ...,\n",
      "        [ 0.6441,  0.5729,  0.4969,  ..., -0.0568, -0.0753, -0.4971],\n",
      "        [ 0.5640,  0.6397,  0.5771,  ..., -0.0568, 13.2853, -0.4971],\n",
      "        [ 0.5640,  0.6397,  0.5771,  ..., -0.0568, 13.2853, -0.4971]])\n"
     ]
    }
   ],
   "source": [
    "datasets: TargetContainer[DatasetContainer] = TargetContainer()\n",
    "for _key in (\"PI\", \"COSMO\", \"Chi\"):\n",
    "    _desc: pd.DataFrame = getattr(desc_container, _key)\n",
    "    for _train_or_test in (\"train\", \"test\"):\n",
    "        _index = getattr(index_containers[_key], _train_or_test)\n",
    "        _X = np.concatenate(\n",
    "            (\n",
    "                selector_X_ff_all.transform(\n",
    "                    _desc.loc[_index, desc_names_forcefield]\n",
    "                ),\n",
    "                scaler_X_rdkit_all.transform(\n",
    "                    _desc.loc[_index, desc_names_rdkit]\n",
    "                ),\n",
    "            ),\n",
    "            axis=1,\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        assert _X.shape[1] == len(desc_names_use)\n",
    "\n",
    "        _X_polymer = torch.tensor(\n",
    "            _X[\n",
    "                :,\n",
    "                np.char.startswith(\n",
    "                    desc_names_use,\n",
    "                    \"Polymer_\",\n",
    "                ),\n",
    "            ],\n",
    "            device=device,\n",
    "        )\n",
    "        _X_solvent = torch.tensor(\n",
    "            _X[\n",
    "                :,\n",
    "                np.char.startswith(\n",
    "                    desc_names_use,\n",
    "                    \"Solvent_\",\n",
    "                ),\n",
    "            ],\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        _y_scaled = torch.tensor(\n",
    "            y_scaled_container[_key].loc[_index].values,\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        setattr(\n",
    "            getattr(datasets, _key),\n",
    "            _train_or_test,\n",
    "            DatasetContainer(\n",
    "                train=DatasetLikeObject(\n",
    "                    x_polymer=_X_polymer, x_solvent=_X_solvent, x_temp=, y=_y_scaled,\n",
    "                )\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7dc4e-a58a-45c5-a2c3-0b9a86811c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion_container = TargetContainer(\n",
    "    PI=nn.BCELoss(), COSMO=nn.MSELoss(), Chi=nn.MSELoss()\n",
    ")\n",
    "# dim_in_p = len(dname_p)\n",
    "# dim_in_s = len(dname_s)\n",
    "\n",
    "\n",
    "# HACK: replace random sampling to optuna\n",
    "# generate hyperparameters candidates\n",
    "hyper_para = pd.DataFrame(\n",
    "    {\n",
    "        \"alpha1\": rng.uniform(alpha1s[0], alpha1s[1], n_hpara),\n",
    "        \"alpha2\": rng.uniform(alpha2s[0], alpha2s[1], n_hpara),\n",
    "        \"dim_out\": rng.integers(dim_outs[0], dim_outs[1], n_hpara),\n",
    "        \"lr\": rng.uniform(learning_rates[0], learning_rates[1], n_hpara),\n",
    "    }\n",
    ")\n",
    "\n",
    "os.makedirs(dir_base, exist_ok=True)\n",
    "hyper_para.to_csv(f\"{dir_base}/list_hyperparameters.csv\")\n",
    "\n",
    "\n",
    "XS0_P_TE = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_S_TE = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_T_TE = torch.zeros(\n",
    "    idx_split_s0[\"idx_te\"].shape[0], temp_dim, device=device\n",
    ")\n",
    "YS0_TE = torch.tensor(\n",
    "    y_s0.loc[idx_split_s0[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS_P_TE = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_S_TE = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_T_TE = torch.tensor(\n",
    "    temp_s_s.loc[idx_split_s[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YS_TE = torch.tensor(\n",
    "    y_s_s.loc[idx_split_s[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XT_P_TE = torch.tensor(\n",
    "    desc_t_s.loc[idx_split_t[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XT_S_TE = torch.tensor(\n",
    "    desc_t_s.loc[idx_split_t[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XT_T_TE = torch.tensor(\n",
    "    temp_t_s.loc[idx_split_t[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YT_TE = torch.tensor(\n",
    "    y_t_s.loc[idx_split_t[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS0_P_TR = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_S_TR = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_T_TR = torch.zeros(\n",
    "    idx_split_s0[\"idx_tr\"].shape[0], temp_dim, device=device\n",
    ")\n",
    "YS0_TR = torch.tensor(\n",
    "    y_s0.loc[idx_split_s0[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS_P_TR = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_S_TR = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_T_TR = torch.tensor(\n",
    "    temp_s_s.loc[idx_split_s[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YS_TR = torch.tensor(\n",
    "    y_s_s.loc[idx_split_s[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68267516-ec67-46de-a46c-b169b6e9666c",
   "metadata": {},
   "source": [
    "Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterGenerator(object):\n",
    "    \"\"\"\n",
    "    Generator for parameter set generating.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        **kwargs: Union[Any, Sequence, Callable, dict],\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed\n",
    "            Numpy random seed.\n",
    "        kwargs\n",
    "            Parameter candidate.\n",
    "        \"\"\"\n",
    "        if len(kwargs) == 0:\n",
    "            raise RuntimeError(\"need parameter candidate\")\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.tuples = dict()\n",
    "        self.funcs = dict()\n",
    "        self.dicts = dict()\n",
    "        self.others = dict()\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, (tuple, list, np.ndarray, pd.Series)):\n",
    "                self.tuples[k] = v\n",
    "            elif callable(v):\n",
    "                self.funcs[k] = v\n",
    "            elif isinstance(v, dict):\n",
    "                repeat = v[\"repeat\"]\n",
    "                self.dicts[k] = v\n",
    "\n",
    "                if isinstance(repeat, str):\n",
    "                    if repeat in self.tuples:\n",
    "                        self.tuples[repeat] = self.tuples.pop(repeat)\n",
    "                    if repeat in self.dicts:\n",
    "                        self.dicts[repeat] = self.dicts.pop(repeat)\n",
    "                    if repeat in self.funcs:\n",
    "                        self.funcs[repeat] = self.funcs.pop(repeat)\n",
    "            else:\n",
    "                self.others[k] = v\n",
    "\n",
    "    def __call__(self, num: int, *, factory=None):\n",
    "        for _ in range(num):\n",
    "            tmp = {}\n",
    "            for k, v in self.tuples.items():\n",
    "                tmp[k] = self._gen(v)\n",
    "\n",
    "            for k, v in self.funcs.items():\n",
    "                tmp[k] = v()\n",
    "\n",
    "            for k, v in reversed(self.dicts.items()):\n",
    "                data = v[\"data\"]\n",
    "                repeat = v[\"repeat\"]\n",
    "                if \"replace\" in v:\n",
    "                    replace = v[\"replace\"]\n",
    "                else:\n",
    "                    replace = True\n",
    "\n",
    "                if isinstance(repeat, (tuple, list, np.ndarray, pd.Series)):\n",
    "                    repeat = self._gen(repeat)\n",
    "                elif isinstance(repeat, str):\n",
    "                    repeat = len(tmp[repeat])\n",
    "\n",
    "                if isinstance(data, (tuple, list, np.ndarray, pd.Series)):\n",
    "                    tmp[k] = self._gen(data, repeat, replace)\n",
    "                elif callable(data):\n",
    "                    tmp[k] = tuple(data(repeat))\n",
    "\n",
    "            tmp = dict(self.others, **tmp)\n",
    "            if factory is not None:\n",
    "                yield tmp, factory(**tmp)\n",
    "            else:\n",
    "                yield tmp\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen(item: Sequence, repeat: int = None, replace: bool = True):\n",
    "        if repeat is not None:\n",
    "            idx = rng.choice(len(item), repeat, replace=replace)\n",
    "            return tuple(item[i] for i in idx)\n",
    "        else:\n",
    "            idx = rng.choice(len(item))\n",
    "            return item[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cdbfa-0ac0-4142-beb6-1c95a85e1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iCV, (idx_tr, idx_val) in enumerate(zip(idx_trs, idx_vals)):\n",
    "    XT_P_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_TR = torch.tensor(\n",
    "        temp_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_TR = torch.tensor(\n",
    "        y_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    XT_P_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_VA = torch.tensor(\n",
    "        temp_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_VA = torch.tensor(\n",
    "        y_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for ii, h_paras in hyper_para.iterrows():\n",
    "        alpha1 = h_paras[\"alpha1\"]\n",
    "        alpha2 = h_paras[\"alpha2\"]\n",
    "        dim_out = int(h_paras[\"dim_out\"])\n",
    "        learning_rate = h_paras[\"lr\"]\n",
    "\n",
    "        generator_p = ParameterGenerator(\n",
    "            in_features=dim_in_p,\n",
    "            out_features=dim_out * 2,\n",
    "            h_neurons=dict(\n",
    "                data=lambda x: neuron_vector(x, dim_in_p, dim_out * 2),\n",
    "                repeat=[n_NNlayer],\n",
    "            ),\n",
    "            h_activation_funcs=(nn.Sigmoid(),),\n",
    "            h_dropouts=(0.0,),\n",
    "        )\n",
    "\n",
    "        generator_s = ParameterGenerator(\n",
    "            in_features=dim_in_s,\n",
    "            out_features=dim_out * 2,\n",
    "            h_neurons=dict(\n",
    "                data=lambda x: neuron_vector(x, dim_in_s, dim_out * 2),\n",
    "                repeat=[n_NNlayer],\n",
    "            ),\n",
    "            h_activation_funcs=(nn.Sigmoid(),),\n",
    "            h_dropouts=(0.0,),\n",
    "        )\n",
    "\n",
    "        for iM, ((paras_p, model_p), (paras_s, model_s)) in enumerate(\n",
    "            zip(\n",
    "                generator_p(num=1, factory=SequentialLinear),\n",
    "                generator_s(num=1, factory=SequentialLinear),\n",
    "            )\n",
    "        ):\n",
    "            dir_save = f\"{dir_base}/cv{iCV}/hpara{ii}\"\n",
    "            os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "            c_model = ChiModel(model_p, model_s, dim_out)\n",
    "            _ = c_model.to(device)\n",
    "\n",
    "            _ = c_model.train()\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                c_model.parameters(), lr=learning_rate, amsgrad=True\n",
    "            )\n",
    "            scheduler = StepLR(\n",
    "                optimizer, step_size=sch_step_size, gamma=sch_gamma\n",
    "            )\n",
    "\n",
    "            learning_curve = pd.DataFrame()\n",
    "\n",
    "            for t in range(epochs_s):\n",
    "                # mini-batch of training data\n",
    "                kf = KFold(\n",
    "                    n_splits=n_minibatch_PI,\n",
    "                    shuffle=True,\n",
    "                    random_state=rng.integers(2**31 - 1),\n",
    "                )\n",
    "                idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "\n",
    "                # pre-training with PI\n",
    "                for tt, ii_s0 in enumerate(idx_mb_s0):\n",
    "                    _ = c_model.train()\n",
    "\n",
    "                    tmp_source0_train = c_model(\n",
    "                        XS0_P_TR[ii_s0, :],\n",
    "                        XS0_S_TR[ii_s0, :],\n",
    "                        XS0_T_TR[ii_s0, :],\n",
    "                    )\n",
    "                    py_source0_train = tmp_source0_train[:, 0:1]\n",
    "                    loss_source0_train = criterion_source0(\n",
    "                        py_source0_train, YS0_TR[ii_s0, :]\n",
    "                    )\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_source0_train.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    _ = c_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source_train = criterion_source(\n",
    "                            py_source_train, YS_TR\n",
    "                        )\n",
    "                        loss_target_train = (\n",
    "                            criterion_target(py_target_train, YT_TR)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_train = alpha1 * loss_source0_train + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_train\n",
    "                            + (1.0 - alpha2) * loss_target_train\n",
    "                        )\n",
    "\n",
    "                        py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        loss_target_val = (\n",
    "                            criterion_target(py_target_val, YT_VA)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "\n",
    "                        py_source0_test = c_model(\n",
    "                            XS0_P_TE, XS0_S_TE, XS0_T_TE\n",
    "                        )[:, 0:1]\n",
    "                        py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source0_test = criterion_source0(\n",
    "                            py_source0_test, YS0_TE\n",
    "                        )\n",
    "                        loss_source_test = criterion_source(\n",
    "                            py_source_test, YS_TE\n",
    "                        )\n",
    "                        loss_target_test = (\n",
    "                            criterion_target(py_target_test, YT_TE)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_test = alpha1 * loss_source0_test + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_test\n",
    "                            + (1.0 - alpha2) * loss_target_test\n",
    "                        )\n",
    "\n",
    "                    learning_curve = pd.concat(\n",
    "                        [\n",
    "                            learning_curve,\n",
    "                            pd.Series(\n",
    "                                {\n",
    "                                    \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                    \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                    \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                    \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                    \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                    \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                    \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                    \"Loss_Training\": loss_train.item(),\n",
    "                                    \"Loss_Test\": loss_test.item(),\n",
    "                                },\n",
    "                                name=f\"pre_{t}\",\n",
    "                            )\n",
    "                            .to_frame()\n",
    "                            .T,\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "            # main training\n",
    "            best_loss_val = np.inf\n",
    "            for t in range(epochs):\n",
    "                # mini-batch of training data\n",
    "                kf = KFold(\n",
    "                    n_splits=n_minibatch_PI,\n",
    "                    shuffle=True,\n",
    "                    random_state=rng.integers(2**31 - 1),\n",
    "                )\n",
    "                idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "                idx_mb_s, idx_mb_t = [], []\n",
    "                for k in range(n_factor_COSMO):\n",
    "                    kf = KFold(\n",
    "                        n_splits=n_minibatch_COSMO,\n",
    "                        shuffle=True,\n",
    "                        random_state=rng.integers(2**31 - 1),\n",
    "                    )\n",
    "                    idx_mb_s += [x for _, x in kf.split(XS_P_TR)]\n",
    "                for k in range(n_factor_CHI):\n",
    "                    kf = KFold(\n",
    "                        n_splits=n_minibatch_CHI,\n",
    "                        shuffle=True,\n",
    "                        random_state=rng.integers(2**31 - 1),\n",
    "                    )\n",
    "                    idx_mb_t += [x for _, x in kf.split(XT_P_TR)]\n",
    "\n",
    "                for tt, (ii_s0, ii_s, ii_t) in enumerate(\n",
    "                    zip(idx_mb_s0, idx_mb_s, idx_mb_t)\n",
    "                ):\n",
    "                    _ = c_model.train()\n",
    "                    if alpha1 > 0:\n",
    "                        py_source0_train = c_model(\n",
    "                            XS0_P_TR[ii_s0, :],\n",
    "                            XS0_S_TR[ii_s0, :],\n",
    "                            XS0_T_TR[ii_s0, :],\n",
    "                        )[:, 0:1]\n",
    "                        loss_source0_train = criterion_source0(\n",
    "                            py_source0_train, YS0_TR[ii_s0, :]\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_source0_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    if (alpha2 > 0) and (alpha1 < 1):\n",
    "                        if no_COSMO_BN:\n",
    "                            _ = c_model.eval()\n",
    "                        else:\n",
    "                            _ = c_model.train()\n",
    "                        py_source_train = c_model(\n",
    "                            XS_P_TR[ii_s, :],\n",
    "                            XS_S_TR[ii_s, :],\n",
    "                            XS_T_TR[ii_s, :],\n",
    "                        )[:, 1:2]\n",
    "                        loss_source_train = criterion_source(\n",
    "                            py_source_train, YS_TR[ii_s, :]\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_source_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    if (alpha1 < 1) and (alpha2 < 1):\n",
    "                        if no_target_BN:\n",
    "                            _ = c_model.eval()\n",
    "                        else:\n",
    "                            _ = c_model.train()\n",
    "                        py_target_train = c_model(\n",
    "                            XT_P_TR[ii_t, :],\n",
    "                            XT_S_TR[ii_t, :],\n",
    "                            XT_T_TR[ii_t, :],\n",
    "                        )[:, 2:3]\n",
    "                        loss_target_train = (\n",
    "                            criterion_target(py_target_train, YT_TR[ii_t, :])\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_target_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    loss_train = alpha1 * loss_source0_train + (\n",
    "                        1.0 - alpha1\n",
    "                    ) * (\n",
    "                        alpha2 * loss_source_train\n",
    "                        + (1.0 - alpha2) * loss_target_train\n",
    "                    )\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    _ = c_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_target_val = (\n",
    "                            criterion_target(py_target_val, YT_VA)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "\n",
    "                        py_source0_test = c_model(\n",
    "                            XS0_P_TE, XS0_S_TE, XS0_T_TE\n",
    "                        )[:, 0:1]\n",
    "                        py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source0_test = criterion_source0(\n",
    "                            py_source0_test, YS0_TE\n",
    "                        )\n",
    "                        loss_source_test = criterion_source(\n",
    "                            py_source_test, YS_TE\n",
    "                        )\n",
    "                        loss_target_test = (\n",
    "                            criterion_target(py_target_test, YT_TE)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_test = alpha1 * loss_source0_test + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_test\n",
    "                            + (1.0 - alpha2) * loss_target_test\n",
    "                        )\n",
    "\n",
    "                    learning_curve = pd.concat(\n",
    "                        [\n",
    "                            learning_curve,\n",
    "                            pd.Series(\n",
    "                                {\n",
    "                                    \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                    \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                    \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                    \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                    \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                    \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                    \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                    \"Loss_Training\": loss_train.item(),\n",
    "                                    \"Loss_Test\": loss_test.item(),\n",
    "                                },\n",
    "                                name=f\"main_{t}_{tt}\",\n",
    "                            )\n",
    "                            .to_frame()\n",
    "                            .T,\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "                    if (t > burn_in) and (loss_target_val < best_loss_val):\n",
    "                        save_NN(\n",
    "                            paras_p,\n",
    "                            paras_s,\n",
    "                            dim_out,\n",
    "                            c_model,\n",
    "                            f\"{dir_save}/best_loss_target_val.pt\",\n",
    "                        )\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            py_source0_train = c_model(\n",
    "                                XS0_P_TR, XS0_S_TR, XS0_T_TR\n",
    "                            )[:, 0:1]\n",
    "                            py_source_train = c_model(\n",
    "                                XS_P_TR, XS_S_TR, XS_T_TR\n",
    "                            )[:, 1:2]\n",
    "                            py_target_train = c_model(\n",
    "                                XT_P_TR, XT_S_TR, XT_T_TR\n",
    "                            )[:, 2:3]\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s0.loc[idx_split_s0[\"idx_tr\"], :],\n",
    "                                pd.Series(\n",
    "                                    py_source0_train.to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                    .flatten(),\n",
    "                                    index=idx_split_s0[\"idx_tr\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source0_train.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s.loc[idx_split_s[\"idx_tr\"], :],\n",
    "                                pd.Series(\n",
    "                                    ys_scaler.inverse_transform(\n",
    "                                        py_source_train.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_s[\"idx_tr\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source_train.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_tr, :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_train.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_tr,\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_train.csv\"\n",
    "                        )\n",
    "\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_val, :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_val.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_val,\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_val.csv\"\n",
    "                        )\n",
    "\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s0.loc[idx_split_s0[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    py_source0_test.to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                    .flatten(),\n",
    "                                    index=idx_split_s0[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source0_test.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s.loc[idx_split_s[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    ys_scaler.inverse_transform(\n",
    "                                        py_source_test.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_s[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source_test.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_split_t[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_test.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_t[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_test.csv\"\n",
    "                        )\n",
    "\n",
    "                        best_loss_val = loss_target_val\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            learning_curve.to_csv(f\"{dir_save}/learning_curve.csv\")\n",
    "\n",
    "    print(f\"Finished model {iCV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb38cd-17d4-4fa1-b7e1-7ca84d66b6e6",
   "metadata": {},
   "source": [
    "## Extract CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e857e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(\n",
    "    y_true: Union[np.ndarray, pd.Series], y_pred: Union[np.ndarray, pd.Series]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate most common regression scores.\n",
    "    See Also: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True results.\n",
    "    y_pred\n",
    "        Predicted results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        An :class:`collections.OrderedDict` contains regression scores.\n",
    "        These scores will be calculated: ``mae``, ``mse``, ``rmse``, ``r2``,\n",
    "        ``pearsonr``, ``spearmanr``, ``p_value``, and ``max_ae``\n",
    "    \"\"\"\n",
    "    if len(y_true.shape) != 1:\n",
    "        y_true = y_true.flatten()\n",
    "    if len(y_pred.shape) != 1:\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    maxae = max_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pr, p_val = pearsonr(y_true, y_pred)\n",
    "    sr, _ = spearmanr(y_true, y_pred)\n",
    "    return dict(\n",
    "        mae=mae,\n",
    "        mse=mse,\n",
    "        rmse=rmse,\n",
    "        r2=r2,\n",
    "        pearsonr=pr,\n",
    "        spearmanr=sr,\n",
    "        p_value=p_val,\n",
    "        max_ae=maxae,\n",
    "    )\n",
    "\n",
    "\n",
    "def classification_metrics(\n",
    "    y_true: Union[np.ndarray, pd.DataFrame, pd.Series],\n",
    "    y_pred: Union[np.ndarray, pd.Series],\n",
    "    *,\n",
    "    average: Optional[Sequence[Literal[\"weighted\", \"micro\", \"macro\"]]] = (\n",
    "        \"weighted\",\n",
    "        \"micro\",\n",
    "        \"macro\",\n",
    "    ),\n",
    "    labels=None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate most common classification scores.\n",
    "    See also: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True results.\n",
    "    y_pred\n",
    "        Predicted results.\n",
    "    average\n",
    "        This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned.\n",
    "        Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "        binary:\n",
    "            Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred})\n",
    "            are binary.\n",
    "        micro:\n",
    "            Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "        macro:\n",
    "            Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into\n",
    "            account.\n",
    "        weighted:\n",
    "            Calculate metrics for each label, and find their average weighted by support (the number of true instances\n",
    "            for each label). This alters ``macro`` to account for label imbalance; it can result in an F-score that is\n",
    "            not between precision and recall.\n",
    "    labels\n",
    "        The set of labels to include when average != ``binary``, and their order if average is None.\n",
    "        Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority\n",
    "        negative class, while labels not present in the data will result in 0 components in a macro average.\n",
    "        For multilabel targets, labels are column indices.\n",
    "        By default, all labels in y_true and y_pred are used in sorted order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        An :class:`collections.OrderedDict` contains classification scores.\n",
    "        These scores will always contains ``accuracy``, ``f1``, ``precision`` and ``recall``.\n",
    "        For multilabel targets, based on the selection of the ``average`` parameter, the **weighted**, **micro**,\n",
    "        and **macro** scores of ``f1`, ``precision``, and ``recall`` will be calculated.\n",
    "    \"\"\"\n",
    "    if average is not None and len(average) == 0:\n",
    "        raise ValueError(\"need average\")\n",
    "\n",
    "    if len(y_true.shape) != 1:\n",
    "        y_true = np.argmax(y_true, 1)\n",
    "    if len(y_pred.shape) != 1:\n",
    "        y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    ret = dict(accuracy=accuracy_score(y_true, y_pred))\n",
    "\n",
    "    ret.update(\n",
    "        f1=f1_score(y_true, y_pred, average=None, labels=labels),\n",
    "        precision=precision_score(y_true, y_pred, average=None, labels=labels),\n",
    "        recall=recall_score(y_true, y_pred, average=None, labels=labels),\n",
    "    )\n",
    "\n",
    "    if \"binary\" in average:\n",
    "        ret.update(\n",
    "            binary_f1=f1_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "            binary_precision=precision_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "            binary_recall=recall_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"micro\" in average:\n",
    "        ret.update(\n",
    "            micro_f1=f1_score(y_true, y_pred, average=\"micro\", labels=labels),\n",
    "            micro_precision=precision_score(\n",
    "                y_true, y_pred, average=\"micro\", labels=labels\n",
    "            ),\n",
    "            micro_recall=recall_score(\n",
    "                y_true, y_pred, average=\"micro\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"macro\" in average:\n",
    "        ret.update(\n",
    "            macro_f1=f1_score(y_true, y_pred, average=\"macro\", labels=labels),\n",
    "            macro_precision=precision_score(\n",
    "                y_true, y_pred, average=\"macro\", labels=labels\n",
    "            ),\n",
    "            macro_recall=recall_score(\n",
    "                y_true, y_pred, average=\"macro\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"weighted\" in average:\n",
    "        ret.update(\n",
    "            weighted_f1=f1_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "            weighted_precision=precision_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "            weighted_recall=recall_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"samples\" in average:\n",
    "        ret.update(\n",
    "            samples_f1=f1_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "            samples_precision=precision_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "            samples_recall=recall_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b767ab-0e3a-4ec2-91a9-977e748270d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DIRPATH_LOAD = f'hyper_groupCV/testset_{IDX_CV_TEST}'\n",
    "DIRPATH_LOAD = dir_base\n",
    "DIRPATH_LOAD = [x for x in os.listdir(dir_base) if x[:2] == \"cv\"]\n",
    "DIRPATH_LOAD.sort(key=lambda item: int(item[2:]))\n",
    "\n",
    "hyper_para = pd.read_csv(f\"{dir_base}/list_hyperparameters.csv\", index_col=0)\n",
    "x_lim_range = [0, epochs * n_minibatch_PI]  # plot only main training parts\n",
    "\n",
    "df_summary = pd.DataFrame()\n",
    "for dirL in DIRPATH_LOAD:\n",
    "    mdl_list = [\n",
    "        x for x in os.listdir(f\"{dir_base}/{dirL}\") if x[:5] == \"hpara\"\n",
    "    ]\n",
    "    mdl_list.sort(key=lambda item: int(item[5:]))\n",
    "    for fn in mdl_list:\n",
    "        learning_curve = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/learning_curve.csv\", index_col=0\n",
    "        )\n",
    "        idx_list = [\n",
    "            x\n",
    "            for x in learning_curve.index\n",
    "            if (x[:5] == \"main_\") and (int(x.split(\"_\")[1]) >= burn_in)\n",
    "        ]\n",
    "        idx = learning_curve[\"Loss_Target_Validation\"].loc[idx_list].idxmin()\n",
    "        tmp_row = (\n",
    "            learning_curve.loc[idx, :].rename(f\"{dirL}_{fn}\").to_frame().T\n",
    "        )\n",
    "        tmp_row[\"cv\"] = int(dirL[2:])\n",
    "        tmp_row[\"n_epoch\"] = int(idx.split(\"_\")[1])\n",
    "        tmp_row[\"hpara\"] = int(fn[5:])\n",
    "        tmp_row[hyper_para.columns] = hyper_para.loc[\n",
    "            tmp_row[\"hpara\"], :\n",
    "        ].values\n",
    "        df_summary = pd.concat([df_summary, tmp_row], axis=0)\n",
    "\n",
    "        # plot learning curves\n",
    "        best_t = int(idx.split(\"_\")[1]) * n_minibatch_PI + int(\n",
    "            idx.split(\"_\")[2]\n",
    "        )\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Target loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Target_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Target_Validation\"].values, label=\"Validation\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Target_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/LearningCurve_TargetLoss.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Source loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Source_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Source_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/LearningCurve_SourceLoss.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Target loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Source0_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Source0_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(\n",
    "            f\"{dir_base}/{dirL}/{fn}/LearningCurve_Source0Loss.png\"\n",
    "        )\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        # plot predictions\n",
    "        df_source0_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source0_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source0_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source0_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_val = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_val.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(\n",
    "                df_source0_train[\"y\"], df_source0_train[\"pred\"] > 0.5\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/confusion_matrix_train.csv\")\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(\n",
    "                df_source0_test[\"y\"], df_source0_test[\"pred\"] > 0.5\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/confusion_matrix_test.csv\")\n",
    "        pd.DataFrame(\n",
    "            classification_metrics(\n",
    "                df_source0_train[\"y\"].values,\n",
    "                (df_source0_train[\"pred\"] > 0.5).values,\n",
    "                average=\"binary\",\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/classification_metrics_train.csv\")\n",
    "        pd.DataFrame(\n",
    "            classification_metrics(\n",
    "                df_source0_test[\"y\"].values,\n",
    "                (df_source0_test[\"pred\"] > 0.5).values,\n",
    "                average=\"binary\",\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/classification_metrics_test.csv\")\n",
    "\n",
    "        _ = plt.figure()\n",
    "        _ = plt.xlabel(\"Prediction\")\n",
    "        _ = plt.ylabel(\"Observation\")\n",
    "        _ = plt.scatter(\n",
    "            df_source_train[\"pred\"],\n",
    "            df_source_train[\"y\"],\n",
    "            c=\"b\",\n",
    "            alpha=0.4,\n",
    "            label=\"Train\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_source_test[\"pred\"],\n",
    "            df_source_test[\"y\"],\n",
    "            c=\"r\",\n",
    "            alpha=0.4,\n",
    "            label=\"Test\",\n",
    "        )\n",
    "        xy_min = min(df_source_test.min().min(), df_source_train.min().min())\n",
    "        xy_max = max(df_source_test.max().max(), df_source_train.max().max())\n",
    "        _ = plt.plot(\n",
    "            [xy_min, xy_max], [xy_min, xy_max], color=\"k\", ls=\"--\", alpha=0.5\n",
    "        )\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/P2O_BestVal_Source.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        source_summary = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_source_train[\"y\"].values,\n",
    "                        df_source_train[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_source_test[\"y\"].values,\n",
    "                        df_source_test[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        source_summary.columns = [\"train\", \"test\"]\n",
    "        source_summary.to_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/PredSummary_BestVal_Source.csv\"\n",
    "        )\n",
    "\n",
    "        _ = plt.figure()\n",
    "        _ = plt.xlabel(\"Prediction\")\n",
    "        _ = plt.ylabel(\"Observation\")\n",
    "        _ = plt.scatter(\n",
    "            df_target_train[\"pred\"],\n",
    "            df_target_train[\"y\"],\n",
    "            c=\"b\",\n",
    "            alpha=0.2,\n",
    "            label=\"Train\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_target_val[\"pred\"],\n",
    "            df_target_val[\"y\"],\n",
    "            c=\"g\",\n",
    "            alpha=0.3,\n",
    "            label=\"Val\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_target_test[\"pred\"],\n",
    "            df_target_test[\"y\"],\n",
    "            c=\"r\",\n",
    "            alpha=0.4,\n",
    "            label=\"Test\",\n",
    "        )\n",
    "        xy_min = min(\n",
    "            df_target_test.min().min(),\n",
    "            df_target_val.min().min(),\n",
    "            df_target_train.min().min(),\n",
    "        )\n",
    "        xy_max = max(\n",
    "            df_target_test.max().max(),\n",
    "            df_target_val.max().max(),\n",
    "            df_target_train.max().max(),\n",
    "        )\n",
    "        _ = plt.plot(\n",
    "            [xy_min, xy_max], [xy_min, xy_max], color=\"k\", ls=\"--\", alpha=0.5\n",
    "        )\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/P2O_BestVal_Target.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        target_summary = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_train[\"y\"].values,\n",
    "                        df_target_train[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_val[\"y\"].values, df_target_val[\"pred\"].values\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_test[\"y\"].values,\n",
    "                        df_target_test[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        target_summary.columns = [\"train\", \"val\", \"test\"]\n",
    "        target_summary.to_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/PredSummary_BestVal_Target.csv\"\n",
    "        )\n",
    "\n",
    "df_summary.sort_values(by=\"Loss_Target_Validation\")\n",
    "df_summary.groupby(\"hpara\").median().sort_values(\"Loss_Target_Validation\")\n",
    "df_summary.groupby(\"hpara\").std().sort_values(\"Loss_Target_Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f21aaf-8f1f-4fe0-a71b-4cc98045264e",
   "metadata": {},
   "source": [
    "Plot CV training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6d63e-63e3-4d59-b971-bfce7d11481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = df_summary.iloc[:, :9].boxplot()\n",
    "_ = plt.figure()\n",
    "_ = df_summary.iloc[:, 4:7].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdbc34-8572-4508-860a-93d5be9eb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = sns.boxplot(data=df_summary, x=\"cv\", y=\"Loss_Target_Validation\")\n",
    "_ = plt.figure()\n",
    "_ = sns.boxplot(data=df_summary, x=\"cv\", y=\"Loss_Target_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693aa55-61a9-4571-aec4-98d13c137f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Target_Validation\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (target-val)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Source0_Test\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (PoLyInfo-test)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Source_Test\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (COSMO-test)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f21f3-318e-482a-8729-d00f42a73f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source0_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source0_Test\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source_Test\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Validation\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Test\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5e6b4-ebe7-422c-9e1d-3042dd2f1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Source0_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af7002-4f12-4612-babb-0d368f9ec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Source_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee2002-61d9-41a2-b1d3-b0d2880c349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Training\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca0d47-863c-4b95-83a0-224015c10566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Validation\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b69c6-2a7c-4cdb-ab6b-84668f2064cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28052d43-6d27-47b2-aa15-5a8f9cc2027f",
   "metadata": {},
   "source": [
    "## Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cd73f-a1c0-40d0-8aa6-fcbed0873c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base2 = \"final_models/\" + dir_base.split(\"/\")[1]\n",
    "os.makedirs(dir_base2, exist_ok=True)\n",
    "\n",
    "poly_group = data_Chi.loc[\n",
    "    idx_split_t[\"idx_tr\"], COLNAME_SOLUBILITY_POLYMER_PAIR\n",
    "]\n",
    "\n",
    "gp_split = GroupShuffleSplit(\n",
    "    n_splits=n_final_model,\n",
    "    test_size=test_ratio_final,\n",
    "    random_state=rng.integers(2**31 - 1),\n",
    ")\n",
    "idx_trs_fin, idx_vals_fin = [], []\n",
    "\n",
    "for idx_tr, idx_val in gp_split.split(\n",
    "    y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]], groups=poly_group.to_list()\n",
    "):\n",
    "    idx_trs_fin.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_tr].index.values\n",
    "    )\n",
    "    idx_vals_fin.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_val].index.values\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0697e71-a77a-49fc-9956-3edb134ac148",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_best_para = (\n",
    "    df_summary.groupby(\"hpara\")\n",
    "    .median()\n",
    "    .sort_values(\"Loss_Target_Validation\")\n",
    "    .iloc[0, :][[\"alpha1\", \"alpha2\", \"dim_out\", \"lr\"]]\n",
    ")\n",
    "alpha1 = tmp_best_para[\"alpha1\"]\n",
    "alpha2 = tmp_best_para[\"alpha2\"]\n",
    "dim_out = int(tmp_best_para[\"dim_out\"])\n",
    "learning_rate = tmp_best_para[\"lr\"]\n",
    "\n",
    "alpha1, alpha2, dim_out, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cff3df-45d6-40fd-8964-507562cd245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iCV, (idx_tr, idx_val) in enumerate(zip(idx_trs_fin, idx_vals_fin)):\n",
    "    XT_P_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_TR = torch.tensor(\n",
    "        temp_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_TR = torch.tensor(\n",
    "        y_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    XT_P_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_VA = torch.tensor(\n",
    "        temp_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_VA = torch.tensor(\n",
    "        y_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    generator_p = ParameterGenerator(\n",
    "        in_features=dim_in_p,\n",
    "        out_features=dim_out * 2,\n",
    "        h_neurons=dict(\n",
    "            data=lambda x: neuron_vector(x, dim_in_p, dim_out * 2),\n",
    "            repeat=[n_NNlayer],\n",
    "        ),\n",
    "        h_activation_funcs=(nn.Sigmoid(),),\n",
    "        h_dropouts=(0.0,),\n",
    "    )\n",
    "\n",
    "    generator_s = ParameterGenerator(\n",
    "        in_features=dim_in_s,\n",
    "        out_features=dim_out * 2,\n",
    "        h_neurons=dict(\n",
    "            data=lambda x: neuron_vector(x, dim_in_s, dim_out * 2),\n",
    "            repeat=[n_NNlayer],\n",
    "        ),\n",
    "        h_activation_funcs=(nn.Sigmoid(),),\n",
    "        h_dropouts=(0.0,),\n",
    "    )\n",
    "\n",
    "    for iM, ((paras_p, model_p), (paras_s, model_s)) in enumerate(\n",
    "        zip(\n",
    "            generator_p(num=1, factory=SequentialLinear),\n",
    "            generator_s(num=1, factory=SequentialLinear),\n",
    "        )\n",
    "    ):\n",
    "        dir_save = f\"{dir_base2}/model_{iCV}\"\n",
    "        os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "        c_model = Chi_Model(model_p, model_s, dim_out)\n",
    "        _ = c_model.to(device)\n",
    "\n",
    "        _ = c_model.train()\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            c_model.parameters(), lr=learning_rate, amsgrad=True\n",
    "        )\n",
    "        scheduler = StepLR(optimizer, step_size=sch_step_size, gamma=sch_gamma)\n",
    "\n",
    "        learning_curve = pd.DataFrame()\n",
    "\n",
    "        for t in range(epochs_s):\n",
    "            # mini-batch of training data\n",
    "            kf = KFold(\n",
    "                n_splits=n_minibatch_PI,\n",
    "                shuffle=True,\n",
    "                random_state=rng.integers(2**31 - 1),\n",
    "            )\n",
    "            idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "\n",
    "            # pre-training with PI\n",
    "            for tt, ii_s0 in enumerate(idx_mb_s0):\n",
    "                _ = c_model.train()\n",
    "\n",
    "                tmp_source0_train = c_model(\n",
    "                    XS0_P_TR[ii_s0, :], XS0_S_TR[ii_s0, :], XS0_T_TR[ii_s0, :]\n",
    "                )\n",
    "                py_source0_train = tmp_source0_train[:, 0:1]\n",
    "                loss_source0_train = criterion_source0(\n",
    "                    py_source0_train, YS0_TR[ii_s0, :]\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_source0_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                _ = c_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                        :, 1:2\n",
    "                    ]\n",
    "                    py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                        :, 2:3\n",
    "                    ]\n",
    "                    loss_source_train = criterion_source(\n",
    "                        py_source_train, YS_TR\n",
    "                    )\n",
    "                    loss_target_train = (\n",
    "                        criterion_target(py_target_train, YT_TR)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_train = alpha1 * loss_source0_train + (\n",
    "                        1.0 - alpha1\n",
    "                    ) * (\n",
    "                        alpha2 * loss_source_train\n",
    "                        + (1.0 - alpha2) * loss_target_train\n",
    "                    )\n",
    "\n",
    "                    py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[:, 1:2]\n",
    "                    loss_target_val = (\n",
    "                        criterion_target(py_target_val, YT_VA)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "\n",
    "                    py_source0_test = c_model(XS0_P_TE, XS0_S_TE, XS0_T_TE)[\n",
    "                        :, 0:1\n",
    "                    ]\n",
    "                    py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[:, 1:2]\n",
    "                    py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[:, 2:3]\n",
    "                    loss_source0_test = criterion_source0(\n",
    "                        py_source0_test, YS0_TE\n",
    "                    )\n",
    "                    loss_source_test = criterion_source(py_source_test, YS_TE)\n",
    "                    loss_target_test = (\n",
    "                        criterion_target(py_target_test, YT_TE)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_test = alpha1 * loss_source0_test + (1.0 - alpha1) * (\n",
    "                        alpha2 * loss_source_test\n",
    "                        + (1.0 - alpha2) * loss_target_test\n",
    "                    )\n",
    "\n",
    "                learning_curve = pd.concat(\n",
    "                    [\n",
    "                        learning_curve,\n",
    "                        pd.Series(\n",
    "                            {\n",
    "                                \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                \"Loss_Training\": loss_train.item(),\n",
    "                                \"Loss_Test\": loss_test.item(),\n",
    "                            },\n",
    "                            name=f\"pre_{t}\",\n",
    "                        )\n",
    "                        .to_frame()\n",
    "                        .T,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        # main training\n",
    "        best_loss_val = np.inf\n",
    "        for t in range(epochs):\n",
    "            # mini-batch of training data\n",
    "            kf = KFold(n_splits=n_minibatch_PI, shuffle=True)\n",
    "            idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "            idx_mb_s, idx_mb_t = [], []\n",
    "            for k in range(n_factor_COSMO):\n",
    "                kf = KFold(n_splits=n_minibatch_COSMO, shuffle=True)\n",
    "                idx_mb_s += [x for _, x in kf.split(XS_P_TR)]\n",
    "            for k in range(n_factor_CHI):\n",
    "                kf = KFold(n_splits=n_minibatch_CHI, shuffle=True)\n",
    "                idx_mb_t += [x for _, x in kf.split(XT_P_TR)]\n",
    "\n",
    "            for tt, (ii_s0, ii_s, ii_t) in enumerate(\n",
    "                zip(idx_mb_s0, idx_mb_s, idx_mb_t)\n",
    "            ):\n",
    "                c_model.train()\n",
    "                if alpha1 > 0:\n",
    "                    py_source0_train = c_model(\n",
    "                        XS0_P_TR[ii_s0, :],\n",
    "                        XS0_S_TR[ii_s0, :],\n",
    "                        XS0_T_TR[ii_s0, :],\n",
    "                    )[:, 0:1]\n",
    "                    loss_source0_train = criterion_source0(\n",
    "                        py_source0_train, YS0_TR[ii_s0, :]\n",
    "                    )\n",
    "                else:\n",
    "                    loss_source0_train = torch.zeros(1, device=device)\n",
    "\n",
    "                if (alpha2 > 0) and (alpha1 < 1):\n",
    "                    if no_COSMO_BN:\n",
    "                        c_model.eval()\n",
    "                    else:\n",
    "                        c_model.train()\n",
    "                    py_source_train = c_model(\n",
    "                        XS_P_TR[ii_s, :], XS_S_TR[ii_s, :], XS_T_TR[ii_s, :]\n",
    "                    )[:, 1:2]\n",
    "                    loss_source_train = criterion_source(\n",
    "                        py_source_train, YS_TR[ii_s, :]\n",
    "                    )\n",
    "                else:\n",
    "                    loss_source_train = torch.zeros(1, device=device)\n",
    "\n",
    "                if (alpha1 < 1) and (alpha2 < 1):\n",
    "                    if no_target_BN:\n",
    "                        c_model.eval()\n",
    "                    else:\n",
    "                        c_model.train()\n",
    "                    py_target_train = c_model(\n",
    "                        XT_P_TR[ii_t, :], XT_S_TR[ii_t, :], XT_T_TR[ii_t, :]\n",
    "                    )[:, 2:3]\n",
    "                    loss_target_train = (\n",
    "                        criterion_target(py_target_train, YT_TR[ii_t, :])\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                else:\n",
    "                    loss_target_train = torch.zeros(1, device=device)\n",
    "\n",
    "                loss_train = alpha1 * loss_source0_train + (1.0 - alpha1) * (\n",
    "                    alpha2 * loss_source_train\n",
    "                    + (1.0 - alpha2) * loss_target_train\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                c_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[:, 2:3]\n",
    "                    loss_target_val = (\n",
    "                        criterion_target(py_target_val, YT_VA)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "\n",
    "                    py_source0_test = c_model(XS0_P_TE, XS0_S_TE, XS0_T_TE)[\n",
    "                        :, 0:1\n",
    "                    ]\n",
    "                    py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[:, 1:2]\n",
    "                    py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)\n",
    "                    loss_source0_test = criterion_source0(\n",
    "                        py_source0_test, YS0_TE\n",
    "                    )\n",
    "                    loss_source_test = criterion_source(py_source_test, YS_TE)\n",
    "                    loss_target_test = (\n",
    "                        criterion_target(py_target_test[:, 2:3], YT_TE)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_test = alpha1 * loss_source0_test + (1.0 - alpha1) * (\n",
    "                        alpha2 * loss_source_test\n",
    "                        + (1.0 - alpha2) * loss_target_test\n",
    "                    )\n",
    "\n",
    "                learning_curve = pd.concat(\n",
    "                    [\n",
    "                        learning_curve,\n",
    "                        pd.Series(\n",
    "                            {\n",
    "                                \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                \"Loss_Training\": loss_train.item(),\n",
    "                                \"Loss_Test\": loss_test.item(),\n",
    "                            },\n",
    "                            name=f\"main_{t}_{tt}\",\n",
    "                        )\n",
    "                        .to_frame()\n",
    "                        .T,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "                if (t > burn_in) and (loss_target_val < best_loss_val):\n",
    "                    save_NN(\n",
    "                        paras_p,\n",
    "                        paras_s,\n",
    "                        dim_out,\n",
    "                        c_model,\n",
    "                        f\"{dir_save}/best_loss_target_val.pt\",\n",
    "                    )\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        py_source0_train = c_model(\n",
    "                            XS0_P_TR, XS0_S_TR, XS0_T_TR\n",
    "                        )[:, 0:1]\n",
    "                        py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s0.loc[idx_split_s0[\"idx_tr\"], :],\n",
    "                            pd.Series(\n",
    "                                py_source0_train.to(\"cpu\")\n",
    "                                .detach()\n",
    "                                .numpy()\n",
    "                                .flatten(),\n",
    "                                index=idx_split_s0[\"idx_tr\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source0_train.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s.loc[idx_split_s[\"idx_tr\"], :],\n",
    "                            pd.Series(\n",
    "                                ys_scaler.inverse_transform(\n",
    "                                    py_source_train.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_s[\"idx_tr\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source_train.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_tr, :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_train.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_tr,\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_target_train.csv\"\n",
    "                    )\n",
    "\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_val, :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_val.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_val,\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(f\"{dir_save}/best_loss_target_val_target_val.csv\")\n",
    "\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s0.loc[idx_split_s0[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                py_source0_test.to(\"cpu\")\n",
    "                                .detach()\n",
    "                                .numpy()\n",
    "                                .flatten(),\n",
    "                                index=idx_split_s0[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source0_test.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s.loc[idx_split_s[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                ys_scaler.inverse_transform(\n",
    "                                    py_source_test.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_s[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source_test.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_split_t[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_test[:, 2:3]\n",
    "                                    .to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_t[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_target_test.csv\"\n",
    "                    )\n",
    "\n",
    "                    tmp_mat = py_target_test.to(\"cpu\").detach().numpy()\n",
    "                    tmp_mat[:, 1:2] = ys_scaler.inverse_transform(\n",
    "                        tmp_mat[:, 1:2]\n",
    "                    )\n",
    "                    tmp_mat[:, 2:3] = yt_scaler.inverse_transform(\n",
    "                        tmp_mat[:, 2:3]\n",
    "                    )\n",
    "                    tmp_mat = pd.DataFrame(tmp_mat)\n",
    "                    tmp_mat.columns = [\n",
    "                        \"Soluble\",\n",
    "                        \"Chi_COSMO\",\n",
    "                        \"Chi_Exp\",\n",
    "                        \"Z_sol\",\n",
    "                        \"A_COSMO\",\n",
    "                        \"B_COSMO\",\n",
    "                        \"A_Exp\",\n",
    "                        \"B_Exp\",\n",
    "                    ] + [f\"Z_{x}\" for x in range(dim_out)]\n",
    "                    tmp_mat.to_csv(f\"{dir_save}/output_target_test.csv\")\n",
    "\n",
    "                    best_loss_val = loss_target_val\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        learning_curve.to_csv(f\"{dir_save}/learning_curve.csv\")\n",
    "\n",
    "    print(f\"Finished model {iCV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978922cc-fd8b-4efe-88f4-fb11566e51c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
