{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd403cc4-448f-47fd-9785-b281792e8abe",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47e5f2a-2b5b-498b-94db-6d13be73ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Any, Literal\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import pickle as pk\n",
    "from copy import deepcopy\n",
    "\n",
    "if sys.version_info >= (3, 9):\n",
    "    from collections.abc import Sequence, Callable\n",
    "else:\n",
    "    from typing import Sequence, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold, KFold, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    max_error,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449544b-fbe8-4db0-9e11-716d773f88e4",
   "metadata": {},
   "source": [
    "User parameters (recorded the actual values when training the model for the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ed9ffe-b879-4929-a03f-be0fe97ff8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "cvtestidx = (\n",
    "    1  # index of the set of randomly split test data stored in test_cv_idx.pkl\n",
    ")\n",
    "\n",
    "# data splitting\n",
    "test_ratio = 0.2  # ratio of total data in computational Chi data and solubility data used for test data\n",
    "# n_CV_val = 5  # k-fold cross-validation for hyperparameter tuning\n",
    "n_CV_val = 2  # HACK: k-fold cross-validation for hyperparameter tuning\n",
    "test_ratio_final = 0.2  # ratio of training data used for validation during final training after selection of the best hyperparameters\n",
    "\n",
    "# hyperparameters\n",
    "# n_hpara = 100  # number of samples in the hyperparameter space\n",
    "n_hpara = 2  # HACK: number of samples in the hyperparameter space\n",
    "learning_rates = [0.001, 0.01]  # range for learning rates\n",
    "alpha1s = [\n",
    "    0.0,\n",
    "    1.0,\n",
    "]  # range for lambda_c (can be set to zero only for 2-task model excluding PoLyInfo dataset)\n",
    "alpha2s = [\n",
    "    0.0,\n",
    "    1.0,\n",
    "]  # range for lambda_s (can be set to zero only for 2-task model excluding COSMO-RS dataset)\n",
    "dim_outs = [3, 40]  # range for dimension of Z\n",
    "\n",
    "# model training\n",
    "dir_base = f\"hyper_groupCV/MT_testset_{cvtestidx}\"  # output directory for hyperparameter selection\n",
    "# n_final_model = 10  # number of retrained model ensembles after selection of the best hyperparameters\n",
    "n_final_model = 2  # HACK: number of retrained model ensembles after selection of the best hyperparameters\n",
    "\n",
    "n_NNlayer = 3  # number of layer in the sub-network of fully connection MLPs\n",
    "sch_step_size = 10  # step size for learning rate scheduler\n",
    "sch_gamma = 0.5  # gamma parameter for learning rate scheduler\n",
    "epochs_s = 0  # number of pre-training steps based on solubility data (not used in the paper)\n",
    "epochs = 50  # number of max. epoch for the main training\n",
    "burn_in = 1  # burn in epoch (skip this number of epoch when selecting the epoch with the lowest validation loss)\n",
    "\n",
    "n_minibatch_PI = 20  # number of minibatch for solubility data\n",
    "n_minibatch_COSMO = 10  # number of minibatch for computational Chi data\n",
    "n_minibatch_CHI = 5  # number minibatch for experimental Chi data\n",
    "n_factor_CHI = int(n_minibatch_PI / n_minibatch_CHI)\n",
    "n_factor_COSMO = int(n_minibatch_PI / n_minibatch_COSMO)\n",
    "\n",
    "# other internal parameters\n",
    "temp_dim = 1  # take 1 or 2 only. 1: linear temperature dependence only; 2: 1/T^2 term included\n",
    "loss_factor_target = 1  # multiplier to adjust target loss contribution to total loss for model training\n",
    "no_target_BN = True  # True: do not include experimental Chi data for batch normalization in training\n",
    "no_COSMO_BN = (\n",
    "    True  # True: do not include COSMO data for batch normalization in training\n",
    ")\n",
    "seed_hyper = 2022  # random seed for hyperparameter samples\n",
    "rng = np.random.default_rng(seed_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f74c96-2a19-4270-a513-666c3c34c782",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee9f5e7-d88a-4736-8ae3-4b324d6e5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_load = \"./sample_data\"\n",
    "\n",
    "data_PI = pd.read_csv(f\"{dir_load}/data_PI.csv\", index_col=0)\n",
    "data_COSMO = pd.read_csv(f\"{dir_load}/data_COSMO.csv\", index_col=0)\n",
    "data_Chi = pd.read_csv(f\"{dir_load}/data_Chi.csv\", index_col=0)\n",
    "desc_PI = pd.read_csv(f\"{dir_load}/desc_PI.csv\", index_col=0)\n",
    "desc_COSMO = pd.read_csv(f\"{dir_load}/desc_COSMO.csv\", index_col=0)\n",
    "desc_Chi = pd.read_csv(f\"{dir_load}/desc_Chi.csv\", index_col=0)\n",
    "\n",
    "with open(f\"{dir_load}/desc_names.pkl\", \"rb\") as f:\n",
    "    tmp = pk.load(f)\n",
    "dname_p_ff = deepcopy(tmp[\"p_ff\"])\n",
    "dname_p_rd = deepcopy(tmp[\"p_rd\"])\n",
    "dname_s_ff = deepcopy(tmp[\"s_ff\"])\n",
    "dname_s_rd = deepcopy(tmp[\"s_rd\"])\n",
    "\n",
    "with open(f\"{dir_load}/test_cv_idx.pkl\", \"rb\") as f:\n",
    "    tmp = pk.load(f)\n",
    "tmp_idx_trs = deepcopy(tmp[\"train\"])\n",
    "tmp_idx_vals = deepcopy(tmp[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ea7ae-7cd7-4254-943f-41c74348f11e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0bc128-ff59-4f6d-8ca0-9a8b6f590037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952, 238)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(434, 772, 35.98673300165838)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(429, 761, 36.05042016806723)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exp-Chi data splitting\n",
    "idx = cvtestidx\n",
    "idx_split_t = {\n",
    "    \"idx_tr\": deepcopy(tmp_idx_trs[idx]),\n",
    "    \"idx_te\": deepcopy(tmp_idx_vals[idx]),\n",
    "}\n",
    "idx_split_t[\"idx_tr\"].shape[0], idx_split_t[\"idx_te\"].shape[0]\n",
    "\n",
    "# COSMO data splitting (exclude test ps_pair cases first)\n",
    "idx = data_COSMO[\"ps_pair\"].apply(\n",
    "    lambda x: x in data_Chi[\"ps_pair\"].loc[idx_split_t[\"idx_te\"]].values\n",
    ")\n",
    "idx.sum()\n",
    "\n",
    "_, tmp_idx = train_test_split(\n",
    "    idx.index[~idx],\n",
    "    test_size=test_ratio,\n",
    "    random_state=rng.integers(2**31 - 1),\n",
    ")\n",
    "idx.loc[tmp_idx] = True\n",
    "idx_split_s = {\n",
    "    \"idx_tr\": data_COSMO.index[~idx],\n",
    "    \"idx_te\": data_COSMO.index[idx],\n",
    "}\n",
    "idx.sum(), (~idx).sum(), idx.sum() / idx.shape[0] * 100\n",
    "\n",
    "# PI data splitting (exclude test polymer cases first)\n",
    "idx = data_PI[\"ps_pair\"].apply(\n",
    "    lambda x: x in data_Chi[\"ps_pair\"].loc[idx_split_t[\"idx_te\"]].values\n",
    ")\n",
    "idx.sum()\n",
    "\n",
    "_, tmp_idx = train_test_split(\n",
    "    idx.index[~idx],\n",
    "    test_size=test_ratio,\n",
    "    random_state=rng.integers(2**31 - 1),\n",
    ")\n",
    "idx.loc[tmp_idx] = True\n",
    "idx_split_s0 = {\"idx_tr\": data_PI.index[~idx], \"idx_te\": data_PI.index[idx]}\n",
    "idx.sum(), (~idx).sum(), idx.sum() / idx.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49341865-bac2-45ad-956c-9d3ef9fef4c3",
   "metadata": {},
   "source": [
    "### Process data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a60e0-c116-4db4-8e30-026507df45a3",
   "metadata": {},
   "source": [
    "Scale descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f078b8-778f-4188-a1bb-4595f20d7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname_rd = np.concatenate([dname_p_rd, dname_s_rd])\n",
    "desc_s0_s = deepcopy(desc_PI)\n",
    "desc_s_s = deepcopy(desc_COSMO)\n",
    "desc_t_s = deepcopy(desc_Chi)\n",
    "\n",
    "tmp_desc = pd.concat(\n",
    "    [\n",
    "        desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_rd],\n",
    "        desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_rd],\n",
    "        desc_t_s.loc[idx_split_t[\"idx_tr\"], dname_rd],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "# tmp_desc = desc_t_s.loc[idx_split_t['idx_tr'],dname_rd]\n",
    "\n",
    "x_scaler = PowerTransformer(method=\"yeo-johnson\").fit(\n",
    "    tmp_desc.drop_duplicates(keep=\"first\")\n",
    ")\n",
    "desc_s0_s[dname_rd] = x_scaler.transform(desc_s0_s[dname_rd])\n",
    "desc_s_s[dname_rd] = x_scaler.transform(desc_s_s[dname_rd])\n",
    "desc_t_s[dname_rd] = x_scaler.transform(desc_t_s[dname_rd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f2a058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polymer_MaxEStateIndex</th>\n",
       "      <th>Polymer_MinEStateIndex</th>\n",
       "      <th>Polymer_MaxAbsEStateIndex</th>\n",
       "      <th>Polymer_MinAbsEStateIndex</th>\n",
       "      <th>Polymer_qed</th>\n",
       "      <th>Polymer_MolWt</th>\n",
       "      <th>Polymer_HeavyAtomMolWt</th>\n",
       "      <th>Polymer_ExactMolWt</th>\n",
       "      <th>Polymer_NumValenceElectrons</th>\n",
       "      <th>Polymer_NumRadicalElectrons</th>\n",
       "      <th>...</th>\n",
       "      <th>Solvent_fr_sulfide</th>\n",
       "      <th>Solvent_fr_sulfonamd</th>\n",
       "      <th>Solvent_fr_sulfone</th>\n",
       "      <th>Solvent_fr_term_acetylene</th>\n",
       "      <th>Solvent_fr_tetrazole</th>\n",
       "      <th>Solvent_fr_thiazole</th>\n",
       "      <th>Solvent_fr_thiocyan</th>\n",
       "      <th>Solvent_fr_thiophene</th>\n",
       "      <th>Solvent_fr_unbrch_alkane</th>\n",
       "      <th>Solvent_fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.764976</td>\n",
       "      <td>0.020314</td>\n",
       "      <td>0.206131</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.197752</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.764976</td>\n",
       "      <td>0.020314</td>\n",
       "      <td>0.206131</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.197752</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.764976</td>\n",
       "      <td>0.020314</td>\n",
       "      <td>0.206131</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.197752</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.764976</td>\n",
       "      <td>0.020314</td>\n",
       "      <td>0.206131</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.197752</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>-0.069415</td>\n",
       "      <td>0.764976</td>\n",
       "      <td>0.020314</td>\n",
       "      <td>0.206131</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.197752</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>-1.224063</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>-1.224063</td>\n",
       "      <td>-0.289050</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.515758</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.540922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>-1.224063</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>-1.224063</td>\n",
       "      <td>-0.289050</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.515758</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.540922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>-1.224063</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>-1.224063</td>\n",
       "      <td>-0.289050</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.515758</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.540922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>-1.224063</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>-1.224063</td>\n",
       "      <td>-0.289050</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.515758</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.540922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>-1.224063</td>\n",
       "      <td>0.317560</td>\n",
       "      <td>-1.224063</td>\n",
       "      <td>-0.289050</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.515758</td>\n",
       "      <td>0.524753</td>\n",
       "      <td>0.540922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.511661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190 rows × 414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Polymer_MaxEStateIndex  Polymer_MinEStateIndex  \\\n",
       "0                  -0.069415                0.838765   \n",
       "1                  -0.069415                0.838765   \n",
       "2                  -0.069415                0.838765   \n",
       "3                  -0.069415                0.838765   \n",
       "4                  -0.069415                0.838765   \n",
       "...                      ...                     ...   \n",
       "1185               -1.224063                0.317560   \n",
       "1186               -1.224063                0.317560   \n",
       "1187               -1.224063                0.317560   \n",
       "1188               -1.224063                0.317560   \n",
       "1189               -1.224063                0.317560   \n",
       "\n",
       "      Polymer_MaxAbsEStateIndex  Polymer_MinAbsEStateIndex  Polymer_qed  \\\n",
       "0                     -0.069415                   0.764976     0.020314   \n",
       "1                     -0.069415                   0.764976     0.020314   \n",
       "2                     -0.069415                   0.764976     0.020314   \n",
       "3                     -0.069415                   0.764976     0.020314   \n",
       "4                     -0.069415                   0.764976     0.020314   \n",
       "...                         ...                        ...          ...   \n",
       "1185                  -1.224063                  -0.289050    -0.970459   \n",
       "1186                  -1.224063                  -0.289050    -0.970459   \n",
       "1187                  -1.224063                  -0.289050    -0.970459   \n",
       "1188                  -1.224063                  -0.289050    -0.970459   \n",
       "1189                  -1.224063                  -0.289050    -0.970459   \n",
       "\n",
       "      Polymer_MolWt  Polymer_HeavyAtomMolWt  Polymer_ExactMolWt  \\\n",
       "0          0.206131                0.253967            0.197752   \n",
       "1          0.206131                0.253967            0.197752   \n",
       "2          0.206131                0.253967            0.197752   \n",
       "3          0.206131                0.253967            0.197752   \n",
       "4          0.206131                0.253967            0.197752   \n",
       "...             ...                     ...                 ...   \n",
       "1185       0.523024                0.515758            0.524753   \n",
       "1186       0.523024                0.515758            0.524753   \n",
       "1187       0.523024                0.515758            0.524753   \n",
       "1188       0.523024                0.515758            0.524753   \n",
       "1189       0.523024                0.515758            0.524753   \n",
       "\n",
       "      Polymer_NumValenceElectrons  Polymer_NumRadicalElectrons  ...  \\\n",
       "0                       -0.204854                          0.0  ...   \n",
       "1                       -0.204854                          0.0  ...   \n",
       "2                       -0.204854                          0.0  ...   \n",
       "3                       -0.204854                          0.0  ...   \n",
       "4                       -0.204854                          0.0  ...   \n",
       "...                           ...                          ...  ...   \n",
       "1185                     0.540922                          0.0  ...   \n",
       "1186                     0.540922                          0.0  ...   \n",
       "1187                     0.540922                          0.0  ...   \n",
       "1188                     0.540922                          0.0  ...   \n",
       "1189                     0.540922                          0.0  ...   \n",
       "\n",
       "      Solvent_fr_sulfide  Solvent_fr_sulfonamd  Solvent_fr_sulfone  \\\n",
       "0                    0.0                   0.0                 0.0   \n",
       "1                    0.0                   0.0                 0.0   \n",
       "2                    0.0                   0.0                 0.0   \n",
       "3                    0.0                   0.0                 0.0   \n",
       "4                    0.0                   0.0                 0.0   \n",
       "...                  ...                   ...                 ...   \n",
       "1185                 0.0                   0.0                 0.0   \n",
       "1186                 0.0                   0.0                 0.0   \n",
       "1187                 0.0                   0.0                 0.0   \n",
       "1188                 0.0                   0.0                 0.0   \n",
       "1189                 0.0                   0.0                 0.0   \n",
       "\n",
       "      Solvent_fr_term_acetylene  Solvent_fr_tetrazole  Solvent_fr_thiazole  \\\n",
       "0                           0.0                   0.0                  0.0   \n",
       "1                           0.0                   0.0                  0.0   \n",
       "2                           0.0                   0.0                  0.0   \n",
       "3                           0.0                   0.0                  0.0   \n",
       "4                           0.0                   0.0                  0.0   \n",
       "...                         ...                   ...                  ...   \n",
       "1185                        0.0                   0.0                  0.0   \n",
       "1186                        0.0                   0.0                  0.0   \n",
       "1187                        0.0                   0.0                  0.0   \n",
       "1188                        0.0                   0.0                  0.0   \n",
       "1189                        0.0                   0.0                  0.0   \n",
       "\n",
       "      Solvent_fr_thiocyan  Solvent_fr_thiophene  Solvent_fr_unbrch_alkane  \\\n",
       "0                     0.0                   0.0                 -0.511661   \n",
       "1                     0.0                   0.0                 -0.511661   \n",
       "2                     0.0                   0.0                 -0.511661   \n",
       "3                     0.0                   0.0                 -0.511661   \n",
       "4                     0.0                   0.0                 -0.511661   \n",
       "...                   ...                   ...                       ...   \n",
       "1185                  0.0                   0.0                 -0.511661   \n",
       "1186                  0.0                   0.0                 -0.511661   \n",
       "1187                  0.0                   0.0                 -0.511661   \n",
       "1188                  0.0                   0.0                 -0.511661   \n",
       "1189                  0.0                   0.0                 -0.511661   \n",
       "\n",
       "      Solvent_fr_urea  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "1185              0.0  \n",
       "1186              0.0  \n",
       "1187              0.0  \n",
       "1188              0.0  \n",
       "1189              0.0  \n",
       "\n",
       "[1190 rows x 414 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_s0_s[dname_rd]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2280481-3e63-465a-b3f2-eb27479bf6ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "Filter out constant descriptors (based on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346ac8c1-5e04-4eb7-b4ad-e7b8e4819093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out constant descriptors\n",
    "dname_rd = np.concatenate([dname_p_rd, dname_s_rd])\n",
    "tmp_desc = pd.concat(\n",
    "    [\n",
    "        desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_rd],\n",
    "        desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_rd],\n",
    "        desc_t_s.loc[idx_split_t[\"idx_tr\"], dname_rd],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "dname_rd_fil = tmp_desc.columns[tmp_desc.std() != 0]\n",
    "dname_p_rd_fil = np.intersect1d(dname_rd_fil, dname_p_rd)\n",
    "dname_s_rd_fil = np.intersect1d(dname_rd_fil, dname_s_rd)\n",
    "\n",
    "dname_ff = np.concatenate([dname_p_ff, dname_s_ff])\n",
    "tmp_desc = pd.concat(\n",
    "    [\n",
    "        desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_ff],\n",
    "        desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_ff],\n",
    "        desc_t_s.loc[idx_split_t[\"idx_tr\"], dname_ff],\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "dname_ff_fil = tmp_desc.columns[tmp_desc.std() != 0]\n",
    "dname_p_ff_fil = np.intersect1d(dname_ff_fil, dname_p_ff)\n",
    "dname_s_ff_fil = np.intersect1d(dname_ff_fil, dname_s_ff)\n",
    "\n",
    "dname_p = np.concatenate([dname_p_ff_fil, dname_p_rd_fil])\n",
    "dname_s = np.concatenate([dname_s_ff_fil, dname_s_rd_fil])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d39bf-2df9-46c8-aa69-065b8619aada",
   "metadata": {},
   "source": [
    "Setup and scale y values and temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1876c0f4-f6ce-4782-a71c-cb7a0d9f17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s0 = data_PI[[\"soluble\"]]\n",
    "y_s0.columns = [\"y\"]\n",
    "y_s = data_COSMO[[\"chi\"]]\n",
    "y_s.columns = [\"y\"]\n",
    "y_t = data_Chi[[\"chi\"]]\n",
    "y_t.columns = [\"y\"]\n",
    "\n",
    "if temp_dim == 1:\n",
    "    temp_s = 1 / (data_COSMO[[\"temp\"]] + 273.15)\n",
    "    temp_s.columns = [\"T1\"]\n",
    "    temp_t = 1 / (data_Chi[[\"temp\"]] + 273.15)\n",
    "    temp_t.columns = [\"T1\"]\n",
    "elif temp_dim == 2:\n",
    "    temp_s = pd.concat(\n",
    "        [\n",
    "            1 / (data_COSMO[[\"temp\"]] + 273.15),\n",
    "            (data_COSMO[[\"temp\"]] + 273.15) ** (-2),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    temp_s.columns = [\"T1\", \"T2\"]\n",
    "    temp_t = pd.concat(\n",
    "        [\n",
    "            1 / (data_Chi[[\"temp\"]] + 273.15),\n",
    "            (data_Chi[[\"temp\"]] + 273.15) ** (-2),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    temp_t.columns = [\"T1\", \"T2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5174ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.358063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.358888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.327410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.604347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.556457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>0.311635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>1.928852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>0.035835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>0.684932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>0.217440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y\n",
       "0     0.358063\n",
       "1     0.358888\n",
       "2     0.327410\n",
       "3     0.604347\n",
       "4     0.556457\n",
       "...        ...\n",
       "1615  0.311635\n",
       "2748  1.928852\n",
       "4134  0.035835\n",
       "8367  0.684932\n",
       "8424  0.217440\n",
       "\n",
       "[1206 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac804580-0bf8-4b65-9652-f426c341569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_scaler = StandardScaler().fit(\n",
    "    y_s.loc[idx_split_s[\"idx_tr\"]].reset_index(drop=True)\n",
    ")\n",
    "y_s_s = pd.DataFrame(\n",
    "    ys_scaler.transform(y_s),\n",
    "    **{key: getattr(y_s, key) for key in (\"index\", \"columns\")},\n",
    ")\n",
    "\n",
    "yt_scaler = StandardScaler().fit(\n",
    "    y_t.loc[idx_split_t[\"idx_tr\"]].reset_index(drop=True)\n",
    ")\n",
    "y_t_s = pd.DataFrame(\n",
    "    yt_scaler.transform(y_t),\n",
    "    **{key: getattr(y_t, key) for key in (\"index\", \"columns\")},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284d809d-f564-4ce5-87ad-23cee7421118",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempS_scaler = StandardScaler().fit(temp_s.loc[idx_split_s[\"idx_tr\"], :])\n",
    "temp_s_s = pd.DataFrame(\n",
    "    tempS_scaler.transform(temp_s),\n",
    "    **{key: getattr(temp_s, key) for key in (\"index\", \"columns\")},\n",
    ")\n",
    "\n",
    "tempT_scaler = StandardScaler().fit(temp_t.loc[idx_split_t[\"idx_tr\"], :])\n",
    "temp_t_s = pd.DataFrame(\n",
    "    tempT_scaler.transform(temp_t),\n",
    "    **{key: getattr(temp_t, key) for key in (\"index\", \"columns\")},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d5747-812f-4c56-8e65-9c1b8a0a1731",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91fcbaf0-fdfc-4e9c-ba57-cd903c16fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed linearly reducing pyramid shape\n",
    "def neuron_vector(nL, in_neu, out_neu):\n",
    "    return [\n",
    "        int(x) for x in np.rint(np.linspace(in_neu, out_neu, nL + 2))[1:-1]\n",
    "    ]\n",
    "\n",
    "\n",
    "# module of the final NN\n",
    "if temp_dim == 1:\n",
    "\n",
    "    class Chi_Model(nn.Module):\n",
    "        def __init__(self, sp_mdl_p, sp_mdl_s, dim_ur):\n",
    "            super(Chi_Model, self).__init__()\n",
    "\n",
    "            self.network1 = deepcopy(sp_mdl_p)\n",
    "            self.network2 = deepcopy(sp_mdl_s)\n",
    "\n",
    "            self.out_lin = nn.Linear(dim_ur, 5)\n",
    "            self.out_act = nn.Sigmoid()\n",
    "            self.dim_ur = dim_ur\n",
    "\n",
    "        def forward(self, x1, x2, temp):\n",
    "            ur1 = self.network1(x1)\n",
    "            ur2 = self.network2(x2)\n",
    "\n",
    "            sp = (ur1[:, : self.dim_ur] - ur2[:, : self.dim_ur]) ** 2\n",
    "            r1 = ur1[:, self.dim_ur :] ** 2\n",
    "            r2 = ur2[:, self.dim_ur :] ** 2\n",
    "\n",
    "            z0 = sp - r1 - r2\n",
    "            z = self.out_lin(z0)\n",
    "\n",
    "            z_soluble = self.out_act(z[:, 0:1])\n",
    "\n",
    "            As = z[:, 1:2]\n",
    "            Bs = z[:, 2:3]\n",
    "            z_comp = As + Bs * temp[:, 0:1]\n",
    "\n",
    "            At = z[:, 3:4]\n",
    "            Bt = z[:, 4:5]\n",
    "            z_target = At + Bt * temp[:, 0:1]\n",
    "\n",
    "            y = torch.cat((z_soluble, z_comp, z_target, z, z0), dim=1)\n",
    "\n",
    "            return y\n",
    "\n",
    "elif temp_dim == 2:\n",
    "\n",
    "    class Chi_Model(nn.Module):\n",
    "        def __init__(self, sp_mdl_p, sp_mdl_s, dim_ur):\n",
    "            super(Chi_Model, self).__init__()\n",
    "\n",
    "            self.network1 = deepcopy(sp_mdl_p)\n",
    "            self.network2 = deepcopy(sp_mdl_s)\n",
    "\n",
    "            self.out_lin = nn.Linear(dim_ur, 7)\n",
    "            self.out_act = nn.Sigmoid()\n",
    "            self.dim_ur = dim_ur\n",
    "\n",
    "        def forward(self, x1, x2, temp):\n",
    "            ur1 = self.network1(x1)\n",
    "            ur2 = self.network2(x2)\n",
    "\n",
    "            sp = (ur1[:, : self.dim_ur] - ur2[:, : self.dim_ur]) ** 2\n",
    "            r1 = ur1[:, self.dim_ur :] ** 2\n",
    "            r2 = ur2[:, self.dim_ur :] ** 2\n",
    "\n",
    "            z0 = sp - r1 - r2\n",
    "            z = self.out_lin(z0)\n",
    "\n",
    "            z_soluble = self.out_act(z[:, 0:1])\n",
    "\n",
    "            As = z[:, 1:2]\n",
    "            Bs = z[:, 2:3]\n",
    "            Cs = z[:, 3:4]\n",
    "            z_comp = As + Bs * temp[:, 0:1] + Cs * temp[:, 1:2]\n",
    "\n",
    "            At = z[:, 4:5]\n",
    "            Bt = z[:, 5:6]\n",
    "            Ct = z[:, 6:7]\n",
    "            z_target = At + Bt * temp[:, 0:1] + Ct * temp[:, 1:2]\n",
    "\n",
    "            y = torch.cat((z_soluble, z_comp, z_target, z, z0), dim=1)\n",
    "\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94be9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Base NN layer. This is a wrap around PyTorch.\n",
    "    See here for details: http://pytorch.org/docs/master/nn.html#\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        activation_func: Callable = nn.ReLU(),\n",
    "        normalizer: Union[float, None] = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features:\n",
    "            Size of each input sample.\n",
    "        out_features:\n",
    "            Size of each output sample\n",
    "        bias:\n",
    "            If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        dropout: float\n",
    "            Probability of an element to be zeroed. Default: 0.5\n",
    "        activation_func: func\n",
    "            Activation function.\n",
    "        normalizer: func\n",
    "            Normalization layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalizer = (\n",
    "            None\n",
    "            if not normalizer\n",
    "            else nn.BatchNorm1d(out_features, momentum=normalizer)\n",
    "        )\n",
    "        self.activation = None if not activation_func else activation_func\n",
    "\n",
    "    def forward(self, x):\n",
    "        _out = self.linear(x)\n",
    "        if self.dropout:\n",
    "            _out = self.dropout(_out)\n",
    "        if self.normalizer:\n",
    "            _out = self.normalizer(_out)\n",
    "        if self.activation:\n",
    "            _out = self.activation(_out)\n",
    "        return _out\n",
    "\n",
    "\n",
    "class SequentialLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential model with linear layers and configurable other hype-parameters.\n",
    "    e.g. ``dropout``, ``hidden layers``\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        *,\n",
    "        h_neurons: Union[Sequence[float], Sequence[int]] = (),\n",
    "        h_bias: Union[bool, Sequence[bool]] = True,\n",
    "        h_dropouts: Union[float, Sequence[float]] = 0.0,\n",
    "        h_normalizers: Union[float, None, Sequence[Optional[float]]] = 0.1,\n",
    "        h_activation_funcs: Union[\n",
    "            Callable, None, Sequence[Optional[Callable]]\n",
    "        ] = nn.ReLU(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features\n",
    "            Size of input.\n",
    "        out_features\n",
    "            Size of output.\n",
    "        bias\n",
    "            Enable ``bias`` in input layer.\n",
    "        h_neurons\n",
    "            Number of neurons in hidden layers.\n",
    "            Can be a tuple of floats. In that case,\n",
    "            all these numbers will be used to calculate the neuron numbers.\n",
    "            e.g. (0.5, 0.4, ...) will be expanded as (in_features * 0.5, in_features * 0.4, ...)\n",
    "        h_bias\n",
    "            ``bias`` in hidden layers.\n",
    "        h_dropouts\n",
    "            Probabilities of dropout in hidden layers.\n",
    "        h_normalizers\n",
    "            Momentum of batched normalizers in hidden layers.\n",
    "        h_activation_funcs\n",
    "            Activation functions in hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._h_layers = len(h_neurons)\n",
    "        if self._h_layers > 0:\n",
    "            if isinstance(h_neurons[0], float):\n",
    "                tmp = [in_features]\n",
    "                for i, ratio in enumerate(h_neurons):\n",
    "                    num = math.ceil(in_features * ratio)\n",
    "                    tmp.append(num)\n",
    "                neurons = tuple(tmp)\n",
    "\n",
    "            elif isinstance(h_neurons[0], int):\n",
    "                neurons = (in_features,) + tuple(h_neurons)\n",
    "            else:\n",
    "                raise RuntimeError(\"illegal parameter type of <h_neurons>\")\n",
    "\n",
    "            activation_funcs = self._check_input(h_activation_funcs)\n",
    "            normalizers = self._check_input(h_normalizers)\n",
    "            dropouts = self._check_input(h_dropouts)\n",
    "            bias = (bias,) + self._check_input(h_bias)\n",
    "\n",
    "            for i in range(self._h_layers):\n",
    "                setattr(\n",
    "                    self,\n",
    "                    f\"layer_{i}\",\n",
    "                    LinearLayer(\n",
    "                        in_features=neurons[i],\n",
    "                        out_features=neurons[i + 1],\n",
    "                        bias=bias[i],\n",
    "                        dropout=dropouts[i],\n",
    "                        activation_func=activation_funcs[i],\n",
    "                        normalizer=normalizers[i],\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            self.output = nn.Linear(neurons[-1], out_features, bias[-1])\n",
    "        else:\n",
    "            self.output = nn.Linear(in_features, out_features, bias)\n",
    "\n",
    "    def _check_input(self, i):\n",
    "        if isinstance(i, Sequence):\n",
    "            if len(i) != self._h_layers:\n",
    "                raise RuntimeError(\n",
    "                    f\"number of parameter not consistent with number of layers, \"\n",
    "                    f\"input is {len(i)} but need to be {self._h_layers}\"\n",
    "                )\n",
    "            return tuple(i)\n",
    "        else:\n",
    "            return tuple([i] * self._h_layers)\n",
    "\n",
    "    def forward(self, x: Any) -> Any:\n",
    "        for i in range(self._h_layers):\n",
    "            x = getattr(self, f\"layer_{i}\")(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a38ec1e-2972-4d1f-8f6c-7d7f16e2ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to save and load the NN model\n",
    "def save_NN(paras_p, paras_s, dim_out, c_mdl, file_name):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_p\": paras_p,\n",
    "            \"model_s\": paras_s,\n",
    "            \"chi\": c_mdl.state_dict(),\n",
    "            \"dim_out\": dim_out,\n",
    "        },\n",
    "        file_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_NN(file_name):\n",
    "    tmp_paras = torch.load(file_name)\n",
    "    c_model = Chi_Model(\n",
    "        SequentialLinear(**tmp_paras[\"model_p\"]),\n",
    "        SequentialLinear(**tmp_paras[\"model_s\"]),\n",
    "        tmp_paras[\"dim_out\"],\n",
    "    )\n",
    "    _ = c_model.load_state_dict(tmp_paras[\"chi\"])\n",
    "    return c_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33695b14-7fab-4925-99bc-3e37a9286eea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning with grid search and CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b3e6-967b-4484-86e9-6cede2b680af",
   "metadata": {},
   "source": [
    "Group validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7532b76c-a0ef-4a11-b432-514e786ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_group = data_Chi.loc[idx_split_t[\"idx_tr\"], \"ps_pair\"]\n",
    "\n",
    "gp_cv = GroupKFold(n_splits=n_CV_val)\n",
    "idx_trs, idx_vals = [], []\n",
    "\n",
    "for idx_tr, idx_val in gp_cv.split(\n",
    "    y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]], groups=poly_group.to_list()\n",
    "):\n",
    "    idx_trs.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_tr].index.values\n",
    "    )\n",
    "    idx_vals.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_val].index.values\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480abc3-ab1a-4164-9e3c-b705e7d412d5",
   "metadata": {},
   "source": [
    "Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72d7dc4e-a58a-45c5-a2c3-0b9a86811c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion_source0 = nn.BCELoss()\n",
    "criterion_source = nn.MSELoss()\n",
    "criterion_target = nn.MSELoss()\n",
    "\n",
    "dim_in_p = len(dname_p)\n",
    "dim_in_s = len(dname_s)\n",
    "\n",
    "hyper_para = pd.DataFrame(\n",
    "    {\n",
    "        \"alpha1\": rng.uniform(alpha1s[0], alpha1s[1], n_hpara),\n",
    "        \"alpha2\": rng.uniform(alpha2s[0], alpha2s[1], n_hpara),\n",
    "        \"dim_out\": rng.integers(dim_outs[0], dim_outs[1], n_hpara),\n",
    "        \"lr\": rng.uniform(learning_rates[0], learning_rates[1], n_hpara),\n",
    "    }\n",
    ")\n",
    "\n",
    "os.makedirs(dir_base, exist_ok=True)\n",
    "hyper_para.to_csv(f\"{dir_base}/list_hyperparameters.csv\")\n",
    "\n",
    "XS0_P_TE = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_S_TE = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_T_TE = torch.zeros(\n",
    "    idx_split_s0[\"idx_te\"].shape[0], temp_dim, device=device\n",
    ")\n",
    "YS0_TE = torch.tensor(\n",
    "    y_s0.loc[idx_split_s0[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS_P_TE = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_S_TE = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_T_TE = torch.tensor(\n",
    "    temp_s_s.loc[idx_split_s[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YS_TE = torch.tensor(\n",
    "    y_s_s.loc[idx_split_s[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XT_P_TE = torch.tensor(\n",
    "    desc_t_s.loc[idx_split_t[\"idx_te\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XT_S_TE = torch.tensor(\n",
    "    desc_t_s.loc[idx_split_t[\"idx_te\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XT_T_TE = torch.tensor(\n",
    "    temp_t_s.loc[idx_split_t[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YT_TE = torch.tensor(\n",
    "    y_t_s.loc[idx_split_t[\"idx_te\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS0_P_TR = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_S_TR = torch.tensor(\n",
    "    desc_s0_s.loc[idx_split_s0[\"idx_tr\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS0_T_TR = torch.zeros(\n",
    "    idx_split_s0[\"idx_tr\"].shape[0], temp_dim, device=device\n",
    ")\n",
    "YS0_TR = torch.tensor(\n",
    "    y_s0.loc[idx_split_s0[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "XS_P_TR = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_p].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_S_TR = torch.tensor(\n",
    "    desc_s_s.loc[idx_split_s[\"idx_tr\"], dname_s].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "XS_T_TR = torch.tensor(\n",
    "    temp_s_s.loc[idx_split_s[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")\n",
    "YS_TR = torch.tensor(\n",
    "    y_s_s.loc[idx_split_s[\"idx_tr\"], :].values.astype(\"float\"),\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68267516-ec67-46de-a46c-b169b6e9666c",
   "metadata": {},
   "source": [
    "Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7019b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterGenerator(object):\n",
    "    \"\"\"\n",
    "    Generator for parameter set generating.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        **kwargs: Union[Any, Sequence, Callable, dict],\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed\n",
    "            Numpy random seed.\n",
    "        kwargs\n",
    "            Parameter candidate.\n",
    "        \"\"\"\n",
    "        if len(kwargs) == 0:\n",
    "            raise RuntimeError(\"need parameter candidate\")\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.tuples = dict()\n",
    "        self.funcs = dict()\n",
    "        self.dicts = dict()\n",
    "        self.others = dict()\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, (tuple, list, np.ndarray, pd.Series)):\n",
    "                self.tuples[k] = v\n",
    "            elif callable(v):\n",
    "                self.funcs[k] = v\n",
    "            elif isinstance(v, dict):\n",
    "                repeat = v[\"repeat\"]\n",
    "                self.dicts[k] = v\n",
    "\n",
    "                if isinstance(repeat, str):\n",
    "                    if repeat in self.tuples:\n",
    "                        self.tuples[repeat] = self.tuples.pop(repeat)\n",
    "                    if repeat in self.dicts:\n",
    "                        self.dicts[repeat] = self.dicts.pop(repeat)\n",
    "                    if repeat in self.funcs:\n",
    "                        self.funcs[repeat] = self.funcs.pop(repeat)\n",
    "            else:\n",
    "                self.others[k] = v\n",
    "\n",
    "    def __call__(self, num: int, *, factory=None):\n",
    "        for _ in range(num):\n",
    "            tmp = {}\n",
    "            for k, v in self.tuples.items():\n",
    "                tmp[k] = self._gen(v)\n",
    "\n",
    "            for k, v in self.funcs.items():\n",
    "                tmp[k] = v()\n",
    "\n",
    "            for k, v in reversed(self.dicts.items()):\n",
    "                data = v[\"data\"]\n",
    "                repeat = v[\"repeat\"]\n",
    "                if \"replace\" in v:\n",
    "                    replace = v[\"replace\"]\n",
    "                else:\n",
    "                    replace = True\n",
    "\n",
    "                if isinstance(repeat, (tuple, list, np.ndarray, pd.Series)):\n",
    "                    repeat = self._gen(repeat)\n",
    "                elif isinstance(repeat, str):\n",
    "                    repeat = len(tmp[repeat])\n",
    "\n",
    "                if isinstance(data, (tuple, list, np.ndarray, pd.Series)):\n",
    "                    tmp[k] = self._gen(data, repeat, replace)\n",
    "                elif callable(data):\n",
    "                    tmp[k] = tuple(data(repeat))\n",
    "\n",
    "            tmp = dict(self.others, **tmp)\n",
    "            if factory is not None:\n",
    "                yield tmp, factory(**tmp)\n",
    "            else:\n",
    "                yield tmp\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen(item: Sequence, repeat: int = None, replace: bool = True):\n",
    "        if repeat is not None:\n",
    "            idx = rng.choice(len(item), repeat, replace=replace)\n",
    "            return tuple([item[i] for i in idx])\n",
    "        else:\n",
    "            idx = rng.choice(len(item))\n",
    "            return item[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e12cdbfa-0ac0-4142-beb6-1c95a85e1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished model 0\n",
      "Finished model 1\n"
     ]
    }
   ],
   "source": [
    "for iCV, (idx_tr, idx_val) in enumerate(zip(idx_trs, idx_vals)):\n",
    "    XT_P_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_TR = torch.tensor(\n",
    "        temp_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_TR = torch.tensor(\n",
    "        y_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    XT_P_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_VA = torch.tensor(\n",
    "        temp_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_VA = torch.tensor(\n",
    "        y_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for ii, h_paras in hyper_para.iterrows():\n",
    "        alpha1 = h_paras[\"alpha1\"]\n",
    "        alpha2 = h_paras[\"alpha2\"]\n",
    "        dim_out = int(h_paras[\"dim_out\"])\n",
    "        learning_rate = h_paras[\"lr\"]\n",
    "\n",
    "        generator_p = ParameterGenerator(\n",
    "            in_features=dim_in_p,\n",
    "            out_features=dim_out * 2,\n",
    "            h_neurons=dict(\n",
    "                data=lambda x: neuron_vector(x, dim_in_p, dim_out * 2),\n",
    "                repeat=[n_NNlayer],\n",
    "            ),\n",
    "            h_activation_funcs=(nn.Sigmoid(),),\n",
    "            h_dropouts=(0.0,),\n",
    "        )\n",
    "\n",
    "        generator_s = ParameterGenerator(\n",
    "            in_features=dim_in_s,\n",
    "            out_features=dim_out * 2,\n",
    "            h_neurons=dict(\n",
    "                data=lambda x: neuron_vector(x, dim_in_s, dim_out * 2),\n",
    "                repeat=[n_NNlayer],\n",
    "            ),\n",
    "            h_activation_funcs=(nn.Sigmoid(),),\n",
    "            h_dropouts=(0.0,),\n",
    "        )\n",
    "\n",
    "        for iM, ((paras_p, model_p), (paras_s, model_s)) in enumerate(\n",
    "            zip(\n",
    "                generator_p(num=1, factory=SequentialLinear),\n",
    "                generator_s(num=1, factory=SequentialLinear),\n",
    "            )\n",
    "        ):\n",
    "            dir_save = f\"{dir_base}/cv{iCV}/hpara{ii}\"\n",
    "            os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "            c_model = Chi_Model(model_p, model_s, dim_out)\n",
    "            _ = c_model.to(device)\n",
    "\n",
    "            _ = c_model.train()\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                c_model.parameters(), lr=learning_rate, amsgrad=True\n",
    "            )\n",
    "            scheduler = StepLR(\n",
    "                optimizer, step_size=sch_step_size, gamma=sch_gamma\n",
    "            )\n",
    "\n",
    "            learning_curve = pd.DataFrame()\n",
    "\n",
    "            for t in range(epochs_s):\n",
    "                # mini-batch of training data\n",
    "                kf = KFold(\n",
    "                    n_splits=n_minibatch_PI,\n",
    "                    shuffle=True,\n",
    "                    random_state=rng.integers(2**31 - 1),\n",
    "                )\n",
    "                idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "\n",
    "                # pre-training with PI\n",
    "                for tt, ii_s0 in enumerate(idx_mb_s0):\n",
    "                    _ = c_model.train()\n",
    "\n",
    "                    tmp_source0_train = c_model(\n",
    "                        XS0_P_TR[ii_s0, :],\n",
    "                        XS0_S_TR[ii_s0, :],\n",
    "                        XS0_T_TR[ii_s0, :],\n",
    "                    )\n",
    "                    py_source0_train = tmp_source0_train[:, 0:1]\n",
    "                    loss_source0_train = criterion_source0(\n",
    "                        py_source0_train, YS0_TR[ii_s0, :]\n",
    "                    )\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_source0_train.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    _ = c_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source_train = criterion_source(\n",
    "                            py_source_train, YS_TR\n",
    "                        )\n",
    "                        loss_target_train = (\n",
    "                            criterion_target(py_target_train, YT_TR)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_train = alpha1 * loss_source0_train + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_train\n",
    "                            + (1.0 - alpha2) * loss_target_train\n",
    "                        )\n",
    "\n",
    "                        py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        loss_target_val = (\n",
    "                            criterion_target(py_target_val, YT_VA)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "\n",
    "                        py_source0_test = c_model(\n",
    "                            XS0_P_TE, XS0_S_TE, XS0_T_TE\n",
    "                        )[:, 0:1]\n",
    "                        py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source0_test = criterion_source0(\n",
    "                            py_source0_test, YS0_TE\n",
    "                        )\n",
    "                        loss_source_test = criterion_source(\n",
    "                            py_source_test, YS_TE\n",
    "                        )\n",
    "                        loss_target_test = (\n",
    "                            criterion_target(py_target_test, YT_TE)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_test = alpha1 * loss_source0_test + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_test\n",
    "                            + (1.0 - alpha2) * loss_target_test\n",
    "                        )\n",
    "\n",
    "                    learning_curve = pd.concat(\n",
    "                        [\n",
    "                            learning_curve,\n",
    "                            pd.Series(\n",
    "                                {\n",
    "                                    \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                    \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                    \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                    \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                    \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                    \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                    \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                    \"Loss_Training\": loss_train.item(),\n",
    "                                    \"Loss_Test\": loss_test.item(),\n",
    "                                },\n",
    "                                name=f\"pre_{t}\",\n",
    "                            )\n",
    "                            .to_frame()\n",
    "                            .T,\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "            # main training\n",
    "            best_loss_val = np.inf\n",
    "            for t in range(epochs):\n",
    "                # mini-batch of training data\n",
    "                kf = KFold(\n",
    "                    n_splits=n_minibatch_PI,\n",
    "                    shuffle=True,\n",
    "                    random_state=rng.integers(2**31 - 1),\n",
    "                )\n",
    "                idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "                idx_mb_s, idx_mb_t = [], []\n",
    "                for k in range(n_factor_COSMO):\n",
    "                    kf = KFold(\n",
    "                        n_splits=n_minibatch_COSMO,\n",
    "                        shuffle=True,\n",
    "                        random_state=rng.integers(2**31 - 1),\n",
    "                    )\n",
    "                    idx_mb_s += [x for _, x in kf.split(XS_P_TR)]\n",
    "                for k in range(n_factor_CHI):\n",
    "                    kf = KFold(\n",
    "                        n_splits=n_minibatch_CHI,\n",
    "                        shuffle=True,\n",
    "                        random_state=rng.integers(2**31 - 1),\n",
    "                    )\n",
    "                    idx_mb_t += [x for _, x in kf.split(XT_P_TR)]\n",
    "\n",
    "                for tt, (ii_s0, ii_s, ii_t) in enumerate(\n",
    "                    zip(idx_mb_s0, idx_mb_s, idx_mb_t)\n",
    "                ):\n",
    "                    _ = c_model.train()\n",
    "                    if alpha1 > 0:\n",
    "                        py_source0_train = c_model(\n",
    "                            XS0_P_TR[ii_s0, :],\n",
    "                            XS0_S_TR[ii_s0, :],\n",
    "                            XS0_T_TR[ii_s0, :],\n",
    "                        )[:, 0:1]\n",
    "                        loss_source0_train = criterion_source0(\n",
    "                            py_source0_train, YS0_TR[ii_s0, :]\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_source0_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    if (alpha2 > 0) and (alpha1 < 1):\n",
    "                        if no_COSMO_BN:\n",
    "                            _ = c_model.eval()\n",
    "                        else:\n",
    "                            _ = c_model.train()\n",
    "                        py_source_train = c_model(\n",
    "                            XS_P_TR[ii_s, :],\n",
    "                            XS_S_TR[ii_s, :],\n",
    "                            XS_T_TR[ii_s, :],\n",
    "                        )[:, 1:2]\n",
    "                        loss_source_train = criterion_source(\n",
    "                            py_source_train, YS_TR[ii_s, :]\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_source_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    if (alpha1 < 1) and (alpha2 < 1):\n",
    "                        if no_target_BN:\n",
    "                            _ = c_model.eval()\n",
    "                        else:\n",
    "                            _ = c_model.train()\n",
    "                        py_target_train = c_model(\n",
    "                            XT_P_TR[ii_t, :],\n",
    "                            XT_S_TR[ii_t, :],\n",
    "                            XT_T_TR[ii_t, :],\n",
    "                        )[:, 2:3]\n",
    "                        loss_target_train = (\n",
    "                            criterion_target(py_target_train, YT_TR[ii_t, :])\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_target_train = torch.zeros(1, device=device)\n",
    "\n",
    "                    loss_train = alpha1 * loss_source0_train + (\n",
    "                        1.0 - alpha1\n",
    "                    ) * (\n",
    "                        alpha2 * loss_source_train\n",
    "                        + (1.0 - alpha2) * loss_target_train\n",
    "                    )\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    _ = c_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_target_val = (\n",
    "                            criterion_target(py_target_val, YT_VA)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "\n",
    "                        py_source0_test = c_model(\n",
    "                            XS0_P_TE, XS0_S_TE, XS0_T_TE\n",
    "                        )[:, 0:1]\n",
    "                        py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                        loss_source0_test = criterion_source0(\n",
    "                            py_source0_test, YS0_TE\n",
    "                        )\n",
    "                        loss_source_test = criterion_source(\n",
    "                            py_source_test, YS_TE\n",
    "                        )\n",
    "                        loss_target_test = (\n",
    "                            criterion_target(py_target_test, YT_TE)\n",
    "                            * loss_factor_target\n",
    "                        )\n",
    "                        loss_test = alpha1 * loss_source0_test + (\n",
    "                            1.0 - alpha1\n",
    "                        ) * (\n",
    "                            alpha2 * loss_source_test\n",
    "                            + (1.0 - alpha2) * loss_target_test\n",
    "                        )\n",
    "\n",
    "                    learning_curve = pd.concat(\n",
    "                        [\n",
    "                            learning_curve,\n",
    "                            pd.Series(\n",
    "                                {\n",
    "                                    \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                    \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                    \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                    \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                    \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                    \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                    \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                    \"Loss_Training\": loss_train.item(),\n",
    "                                    \"Loss_Test\": loss_test.item(),\n",
    "                                },\n",
    "                                name=f\"main_{t}_{tt}\",\n",
    "                            )\n",
    "                            .to_frame()\n",
    "                            .T,\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "                    if (t > burn_in) and (loss_target_val < best_loss_val):\n",
    "                        save_NN(\n",
    "                            paras_p,\n",
    "                            paras_s,\n",
    "                            dim_out,\n",
    "                            c_model,\n",
    "                            f\"{dir_save}/best_loss_target_val.pt\",\n",
    "                        )\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            py_source0_train = c_model(\n",
    "                                XS0_P_TR, XS0_S_TR, XS0_T_TR\n",
    "                            )[:, 0:1]\n",
    "                            py_source_train = c_model(\n",
    "                                XS_P_TR, XS_S_TR, XS_T_TR\n",
    "                            )[:, 1:2]\n",
    "                            py_target_train = c_model(\n",
    "                                XT_P_TR, XT_S_TR, XT_T_TR\n",
    "                            )[:, 2:3]\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s0.loc[idx_split_s0[\"idx_tr\"], :],\n",
    "                                pd.Series(\n",
    "                                    py_source0_train.to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                    .flatten(),\n",
    "                                    index=idx_split_s0[\"idx_tr\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source0_train.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s.loc[idx_split_s[\"idx_tr\"], :],\n",
    "                                pd.Series(\n",
    "                                    ys_scaler.inverse_transform(\n",
    "                                        py_source_train.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_s[\"idx_tr\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source_train.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_tr, :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_train.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_tr,\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_train.csv\"\n",
    "                        )\n",
    "\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_val, :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_val.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_val,\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_val.csv\"\n",
    "                        )\n",
    "\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s0.loc[idx_split_s0[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    py_source0_test.to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                    .flatten(),\n",
    "                                    index=idx_split_s0[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source0_test.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_s.loc[idx_split_s[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    ys_scaler.inverse_transform(\n",
    "                                        py_source_test.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_s[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_source_test.csv\"\n",
    "                        )\n",
    "                        pd.concat(\n",
    "                            [\n",
    "                                y_t.loc[idx_split_t[\"idx_te\"], :],\n",
    "                                pd.Series(\n",
    "                                    yt_scaler.inverse_transform(\n",
    "                                        py_target_test.to(\"cpu\")\n",
    "                                        .detach()\n",
    "                                        .numpy()\n",
    "                                    ).flatten(),\n",
    "                                    index=idx_split_t[\"idx_te\"],\n",
    "                                    name=\"pred\",\n",
    "                                ),\n",
    "                            ],\n",
    "                            axis=1,\n",
    "                        ).to_csv(\n",
    "                            f\"{dir_save}/best_loss_target_val_target_test.csv\"\n",
    "                        )\n",
    "\n",
    "                        best_loss_val = loss_target_val\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            learning_curve.to_csv(f\"{dir_save}/learning_curve.csv\")\n",
    "\n",
    "    print(f\"Finished model {iCV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb38cd-17d4-4fa1-b7e1-7ca84d66b6e6",
   "metadata": {},
   "source": [
    "### Extract CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e857e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(\n",
    "    y_true: Union[np.ndarray, pd.Series], y_pred: Union[np.ndarray, pd.Series]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate most common regression scores.\n",
    "    See Also: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True results.\n",
    "    y_pred\n",
    "        Predicted results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        An :class:`collections.OrderedDict` contains regression scores.\n",
    "        These scores will be calculated: ``mae``, ``mse``, ``rmse``, ``r2``,\n",
    "        ``pearsonr``, ``spearmanr``, ``p_value``, and ``max_ae``\n",
    "    \"\"\"\n",
    "    if len(y_true.shape) != 1:\n",
    "        y_true = y_true.flatten()\n",
    "    if len(y_pred.shape) != 1:\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    maxae = max_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pr, p_val = pearsonr(y_true, y_pred)\n",
    "    sr, _ = spearmanr(y_true, y_pred)\n",
    "    return dict(\n",
    "        mae=mae,\n",
    "        mse=mse,\n",
    "        rmse=rmse,\n",
    "        r2=r2,\n",
    "        pearsonr=pr,\n",
    "        spearmanr=sr,\n",
    "        p_value=p_val,\n",
    "        max_ae=maxae,\n",
    "    )\n",
    "\n",
    "\n",
    "def classification_metrics(\n",
    "    y_true: Union[np.ndarray, pd.DataFrame, pd.Series],\n",
    "    y_pred: Union[np.ndarray, pd.Series],\n",
    "    *,\n",
    "    average: Optional[Sequence[Literal[\"weighted\", \"micro\", \"macro\"]]] = (\n",
    "        \"weighted\",\n",
    "        \"micro\",\n",
    "        \"macro\",\n",
    "    ),\n",
    "    labels=None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate most common classification scores.\n",
    "    See also: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true\n",
    "        True results.\n",
    "    y_pred\n",
    "        Predicted results.\n",
    "    average\n",
    "        This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned.\n",
    "        Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "        binary:\n",
    "            Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred})\n",
    "            are binary.\n",
    "        micro:\n",
    "            Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "        macro:\n",
    "            Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into\n",
    "            account.\n",
    "        weighted:\n",
    "            Calculate metrics for each label, and find their average weighted by support (the number of true instances\n",
    "            for each label). This alters ``macro`` to account for label imbalance; it can result in an F-score that is\n",
    "            not between precision and recall.\n",
    "    labels\n",
    "        The set of labels to include when average != ``binary``, and their order if average is None.\n",
    "        Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority\n",
    "        negative class, while labels not present in the data will result in 0 components in a macro average.\n",
    "        For multilabel targets, labels are column indices.\n",
    "        By default, all labels in y_true and y_pred are used in sorted order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrderedDict\n",
    "        An :class:`collections.OrderedDict` contains classification scores.\n",
    "        These scores will always contains ``accuracy``, ``f1``, ``precision`` and ``recall``.\n",
    "        For multilabel targets, based on the selection of the ``average`` parameter, the **weighted**, **micro**,\n",
    "        and **macro** scores of ``f1`, ``precision``, and ``recall`` will be calculated.\n",
    "    \"\"\"\n",
    "    if average is not None and len(average) == 0:\n",
    "        raise ValueError(\"need average\")\n",
    "\n",
    "    if len(y_true.shape) != 1:\n",
    "        y_true = np.argmax(y_true, 1)\n",
    "    if len(y_pred.shape) != 1:\n",
    "        y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    ret = dict(accuracy=accuracy_score(y_true, y_pred))\n",
    "\n",
    "    ret.update(\n",
    "        f1=f1_score(y_true, y_pred, average=None, labels=labels),\n",
    "        precision=precision_score(y_true, y_pred, average=None, labels=labels),\n",
    "        recall=recall_score(y_true, y_pred, average=None, labels=labels),\n",
    "    )\n",
    "\n",
    "    if \"binary\" in average:\n",
    "        ret.update(\n",
    "            binary_f1=f1_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "            binary_precision=precision_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "            binary_recall=recall_score(\n",
    "                y_true, y_pred, average=\"binary\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"micro\" in average:\n",
    "        ret.update(\n",
    "            micro_f1=f1_score(y_true, y_pred, average=\"micro\", labels=labels),\n",
    "            micro_precision=precision_score(\n",
    "                y_true, y_pred, average=\"micro\", labels=labels\n",
    "            ),\n",
    "            micro_recall=recall_score(\n",
    "                y_true, y_pred, average=\"micro\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"macro\" in average:\n",
    "        ret.update(\n",
    "            macro_f1=f1_score(y_true, y_pred, average=\"macro\", labels=labels),\n",
    "            macro_precision=precision_score(\n",
    "                y_true, y_pred, average=\"macro\", labels=labels\n",
    "            ),\n",
    "            macro_recall=recall_score(\n",
    "                y_true, y_pred, average=\"macro\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"weighted\" in average:\n",
    "        ret.update(\n",
    "            weighted_f1=f1_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "            weighted_precision=precision_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "            weighted_recall=recall_score(\n",
    "                y_true, y_pred, average=\"weighted\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if \"samples\" in average:\n",
    "        ret.update(\n",
    "            samples_f1=f1_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "            samples_precision=precision_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "            samples_recall=recall_score(\n",
    "                y_true, y_pred, average=\"samples\", labels=labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b767ab-0e3a-4ec2-91a9-977e748270d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([2], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m tmp_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(idx\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     30\u001b[0m tmp_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpara\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fn[\u001b[38;5;241m5\u001b[39m:])\n\u001b[0;32m---> 31\u001b[0m tmp_row[hyper_para\u001b[38;5;241m.\u001b[39mcolumns] \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_para\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhpara\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     34\u001b[0m df_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_summary, tmp_row], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# plot learning curves\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mtl311/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([2], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# dir_load = f'hyper_groupCV/testset_{cvtestidx}'\n",
    "dir_load = dir_base\n",
    "dir_load = [x for x in os.listdir(dir_base) if x[:2] == \"cv\"]\n",
    "dir_load.sort(key=lambda item: int(item[2:]))\n",
    "\n",
    "hyper_para = pd.read_csv(f\"{dir_base}/list_hyperparameters.csv\", index_col=0)\n",
    "x_lim_range = [0, epochs * n_minibatch_PI]  # plot only main training parts\n",
    "\n",
    "df_summary = pd.DataFrame()\n",
    "for dirL in dir_load:\n",
    "    mdl_list = [\n",
    "        x for x in os.listdir(f\"{dir_base}/{dirL}\") if x[:5] == \"hpara\"\n",
    "    ]\n",
    "    mdl_list.sort(key=lambda item: int(item[5:]))\n",
    "    for fn in mdl_list:\n",
    "        learning_curve = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/learning_curve.csv\", index_col=0\n",
    "        )\n",
    "        idx_list = [\n",
    "            x\n",
    "            for x in learning_curve.index\n",
    "            if (x[:5] == \"main_\") and (int(x.split(\"_\")[1]) >= burn_in)\n",
    "        ]\n",
    "        idx = learning_curve[\"Loss_Target_Validation\"].loc[idx_list].idxmin()\n",
    "        tmp_row = (\n",
    "            learning_curve.loc[idx, :].rename(f\"{dirL}_{fn}\").to_frame().T\n",
    "        )\n",
    "        tmp_row[\"cv\"] = int(dirL[2:])\n",
    "        tmp_row[\"n_epoch\"] = int(idx.split(\"_\")[1])\n",
    "        tmp_row[\"hpara\"] = int(fn[5:])\n",
    "        tmp_row[hyper_para.columns] = hyper_para.loc[\n",
    "            tmp_row[\"hpara\"], :\n",
    "        ].values\n",
    "        df_summary = pd.concat([df_summary, tmp_row], axis=0)\n",
    "\n",
    "        # plot learning curves\n",
    "        best_t = int(idx.split(\"_\")[1]) * n_minibatch_PI + int(\n",
    "            idx.split(\"_\")[2]\n",
    "        )\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Target loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Target_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Target_Validation\"].values, label=\"Validation\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Target_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/LearningCurve_TargetLoss.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Source loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Source_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Source_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/LearningCurve_SourceLoss.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        _ = plt.xlabel(\"epoch\")\n",
    "        _ = plt.ylabel(\"Target loss\")\n",
    "        _ = plt.plot(\n",
    "            learning_curve[\"Loss_Source0_Training\"].values, label=\"Training\"\n",
    "        )\n",
    "        _ = plt.plot(learning_curve[\"Loss_Source0_Test\"].values, label=\"Test\")\n",
    "        _ = plt.axvline(x=best_t, color=\"r\", ls=\"--\")\n",
    "        _ = plt.xlim(x_lim_range)\n",
    "        _ = plt.ylim([0, 1])\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(\n",
    "            f\"{dir_base}/{dirL}/{fn}/LearningCurve_Source0Loss.png\"\n",
    "        )\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        # plot predictions\n",
    "        df_source0_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source0_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source0_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source0_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_source_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_source_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_train = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_train.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_val = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_val.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        df_target_test = pd.read_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/best_loss_target_val_target_test.csv\",\n",
    "            index_col=0,\n",
    "        )\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(\n",
    "                df_source0_train[\"y\"], df_source0_train[\"pred\"] > 0.5\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/confusion_matrix_train.csv\")\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(\n",
    "                df_source0_test[\"y\"], df_source0_test[\"pred\"] > 0.5\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/confusion_matrix_test.csv\")\n",
    "        pd.DataFrame(\n",
    "            classification_metrics(\n",
    "                df_source0_train[\"y\"].values,\n",
    "                (df_source0_train[\"pred\"] > 0.5).values,\n",
    "                average=\"binary\",\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/classification_metrics_train.csv\")\n",
    "        pd.DataFrame(\n",
    "            classification_metrics(\n",
    "                df_source0_test[\"y\"].values,\n",
    "                (df_source0_test[\"pred\"] > 0.5).values,\n",
    "                average=\"binary\",\n",
    "            )\n",
    "        ).to_csv(f\"{dir_base}/{dirL}/{fn}/classification_metrics_test.csv\")\n",
    "\n",
    "        _ = plt.figure()\n",
    "        _ = plt.xlabel(\"Prediction\")\n",
    "        _ = plt.ylabel(\"Observation\")\n",
    "        _ = plt.scatter(\n",
    "            df_source_train[\"pred\"],\n",
    "            df_source_train[\"y\"],\n",
    "            c=\"b\",\n",
    "            alpha=0.4,\n",
    "            label=\"Train\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_source_test[\"pred\"],\n",
    "            df_source_test[\"y\"],\n",
    "            c=\"r\",\n",
    "            alpha=0.4,\n",
    "            label=\"Test\",\n",
    "        )\n",
    "        xy_min = min(df_source_test.min().min(), df_source_train.min().min())\n",
    "        xy_max = max(df_source_test.max().max(), df_source_train.max().max())\n",
    "        _ = plt.plot(\n",
    "            [xy_min, xy_max], [xy_min, xy_max], color=\"k\", ls=\"--\", alpha=0.5\n",
    "        )\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/P2O_BestVal_Source.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        source_summary = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_source_train[\"y\"].values,\n",
    "                        df_source_train[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_source_test[\"y\"].values,\n",
    "                        df_source_test[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        source_summary.columns = [\"train\", \"test\"]\n",
    "        source_summary.to_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/PredSummary_BestVal_Source.csv\"\n",
    "        )\n",
    "\n",
    "        _ = plt.figure()\n",
    "        _ = plt.xlabel(\"Prediction\")\n",
    "        _ = plt.ylabel(\"Observation\")\n",
    "        _ = plt.scatter(\n",
    "            df_target_train[\"pred\"],\n",
    "            df_target_train[\"y\"],\n",
    "            c=\"b\",\n",
    "            alpha=0.2,\n",
    "            label=\"Train\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_target_val[\"pred\"],\n",
    "            df_target_val[\"y\"],\n",
    "            c=\"g\",\n",
    "            alpha=0.3,\n",
    "            label=\"Val\",\n",
    "        )\n",
    "        _ = plt.scatter(\n",
    "            df_target_test[\"pred\"],\n",
    "            df_target_test[\"y\"],\n",
    "            c=\"r\",\n",
    "            alpha=0.4,\n",
    "            label=\"Test\",\n",
    "        )\n",
    "        xy_min = min(\n",
    "            df_target_test.min().min(),\n",
    "            df_target_val.min().min(),\n",
    "            df_target_train.min().min(),\n",
    "        )\n",
    "        xy_max = max(\n",
    "            df_target_test.max().max(),\n",
    "            df_target_val.max().max(),\n",
    "            df_target_train.max().max(),\n",
    "        )\n",
    "        _ = plt.plot(\n",
    "            [xy_min, xy_max], [xy_min, xy_max], color=\"k\", ls=\"--\", alpha=0.5\n",
    "        )\n",
    "        _ = plt.legend()\n",
    "        _ = plt.savefig(f\"{dir_base}/{dirL}/{fn}/P2O_BestVal_Target.png\")\n",
    "        # _ = plt.show()\n",
    "        _ = plt.close()\n",
    "\n",
    "        target_summary = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_train[\"y\"].values,\n",
    "                        df_target_train[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_val[\"y\"].values, df_target_val[\"pred\"].values\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "                pd.DataFrame.from_dict(\n",
    "                    regression_metrics(\n",
    "                        df_target_test[\"y\"].values,\n",
    "                        df_target_test[\"pred\"].values,\n",
    "                    ),\n",
    "                    orient=\"index\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        target_summary.columns = [\"train\", \"val\", \"test\"]\n",
    "        target_summary.to_csv(\n",
    "            f\"{dir_base}/{dirL}/{fn}/PredSummary_BestVal_Target.csv\"\n",
    "        )\n",
    "\n",
    "df_summary.sort_values(by=\"Loss_Target_Validation\")\n",
    "df_summary.groupby(\"hpara\").median().sort_values(\"Loss_Target_Validation\")\n",
    "df_summary.groupby(\"hpara\").std().sort_values(\"Loss_Target_Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f21aaf-8f1f-4fe0-a71b-4cc98045264e",
   "metadata": {},
   "source": [
    "Plot CV training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6d63e-63e3-4d59-b971-bfce7d11481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = df_summary.iloc[:, :9].boxplot()\n",
    "_ = plt.figure()\n",
    "_ = df_summary.iloc[:, 4:7].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdbc34-8572-4508-860a-93d5be9eb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = sns.boxplot(data=df_summary, x=\"cv\", y=\"Loss_Target_Validation\")\n",
    "_ = plt.figure()\n",
    "_ = sns.boxplot(data=df_summary, x=\"cv\", y=\"Loss_Target_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693aa55-61a9-4571-aec4-98d13c137f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Target_Validation\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (target-val)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Source0_Test\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (PoLyInfo-test)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")\n",
    "\n",
    "_ = plt.figure()\n",
    "_ = plt.scatter(\n",
    "    df_summary[\"Loss_Source_Test\"].values,\n",
    "    df_summary[\"Loss_Target_Test\"].values,\n",
    "    alpha=0.5,\n",
    ")\n",
    "_ = plt.xlabel(\"Loss (COSMO-test)\")\n",
    "_ = plt.ylabel(\"Loss (target-test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f21f3-318e-482a-8729-d00f42a73f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source0_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source0_Test\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Source_Test\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Training\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Validation\"].hist(bins=50)\n",
    "_ = plt.figure()\n",
    "_ = df_summary[\"Loss_Target_Test\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5e6b4-ebe7-422c-9e1d-3042dd2f1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Source0_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af7002-4f12-4612-babb-0d368f9ec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Source_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee2002-61d9-41a2-b1d3-b0d2880c349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Training\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca0d47-863c-4b95-83a0-224015c10566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Validation\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b69c6-2a7c-4cdb-ab6b-84668f2064cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_prop = \"Loss_Target_Test\"\n",
    "for col, vec_col in df_summary.iloc[:, -6:].items():\n",
    "    _ = plt.figure()\n",
    "    _ = plt.scatter(vec_col.values, df_summary[tar_prop].values, alpha=0.5)\n",
    "    _ = plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28052d43-6d27-47b2-aa15-5a8f9cc2027f",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cd73f-a1c0-40d0-8aa6-fcbed0873c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base2 = \"final_models/\" + dir_base.split(\"/\")[1]\n",
    "os.makedirs(dir_base2, exist_ok=True)\n",
    "\n",
    "poly_group = data_Chi.loc[idx_split_t[\"idx_tr\"], \"ps_pair\"]\n",
    "\n",
    "gp_split = GroupShuffleSplit(\n",
    "    n_splits=n_final_model,\n",
    "    test_size=test_ratio_final,\n",
    "    random_state=rng.integers(2**31 - 1),\n",
    ")\n",
    "idx_trs_fin, idx_vals_fin = [], []\n",
    "\n",
    "for idx_tr, idx_val in gp_split.split(\n",
    "    y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]], groups=poly_group.to_list()\n",
    "):\n",
    "    idx_trs_fin.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_tr].index.values\n",
    "    )\n",
    "    idx_vals_fin.append(\n",
    "        y_t[\"y\"].loc[idx_split_t[\"idx_tr\"]].iloc[idx_val].index.values\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0697e71-a77a-49fc-9956-3edb134ac148",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_best_para = (\n",
    "    df_summary.groupby(\"hpara\")\n",
    "    .median()\n",
    "    .sort_values(\"Loss_Target_Validation\")\n",
    "    .iloc[0, :][[\"alpha1\", \"alpha2\", \"dim_out\", \"lr\"]]\n",
    ")\n",
    "alpha1 = tmp_best_para[\"alpha1\"]\n",
    "alpha2 = tmp_best_para[\"alpha2\"]\n",
    "dim_out = int(tmp_best_para[\"dim_out\"])\n",
    "learning_rate = tmp_best_para[\"lr\"]\n",
    "\n",
    "alpha1, alpha2, dim_out, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cff3df-45d6-40fd-8964-507562cd245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iCV, (idx_tr, idx_val) in enumerate(zip(idx_trs_fin, idx_vals_fin)):\n",
    "    XT_P_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_TR = torch.tensor(\n",
    "        desc_t_s.loc[idx_tr, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_TR = torch.tensor(\n",
    "        temp_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_TR = torch.tensor(\n",
    "        y_t_s.loc[idx_tr, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    XT_P_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_p].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_S_VA = torch.tensor(\n",
    "        desc_t_s.loc[idx_val, dname_s].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    XT_T_VA = torch.tensor(\n",
    "        temp_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    YT_VA = torch.tensor(\n",
    "        y_t_s.loc[idx_val, :].values.astype(\"float\"),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    generator_p = ParameterGenerator(\n",
    "        in_features=dim_in_p,\n",
    "        out_features=dim_out * 2,\n",
    "        h_neurons=dict(\n",
    "            data=lambda x: neuron_vector(x, dim_in_p, dim_out * 2),\n",
    "            repeat=[n_NNlayer],\n",
    "        ),\n",
    "        h_activation_funcs=(nn.Sigmoid(),),\n",
    "        h_dropouts=(0.0,),\n",
    "    )\n",
    "\n",
    "    generator_s = ParameterGenerator(\n",
    "        in_features=dim_in_s,\n",
    "        out_features=dim_out * 2,\n",
    "        h_neurons=dict(\n",
    "            data=lambda x: neuron_vector(x, dim_in_s, dim_out * 2),\n",
    "            repeat=[n_NNlayer],\n",
    "        ),\n",
    "        h_activation_funcs=(nn.Sigmoid(),),\n",
    "        h_dropouts=(0.0,),\n",
    "    )\n",
    "\n",
    "    for iM, ((paras_p, model_p), (paras_s, model_s)) in enumerate(\n",
    "        zip(\n",
    "            generator_p(num=1, factory=SequentialLinear),\n",
    "            generator_s(num=1, factory=SequentialLinear),\n",
    "        )\n",
    "    ):\n",
    "        dir_save = f\"{dir_base2}/model_{iCV}\"\n",
    "        os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "        c_model = Chi_Model(model_p, model_s, dim_out)\n",
    "        _ = c_model.to(device)\n",
    "\n",
    "        _ = c_model.train()\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            c_model.parameters(), lr=learning_rate, amsgrad=True\n",
    "        )\n",
    "        scheduler = StepLR(optimizer, step_size=sch_step_size, gamma=sch_gamma)\n",
    "\n",
    "        learning_curve = pd.DataFrame()\n",
    "\n",
    "        for t in range(epochs_s):\n",
    "            # mini-batch of training data\n",
    "            kf = KFold(\n",
    "                n_splits=n_minibatch_PI,\n",
    "                shuffle=True,\n",
    "                random_state=rng.integers(2**31 - 1),\n",
    "            )\n",
    "            idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "\n",
    "            # pre-training with PI\n",
    "            for tt, ii_s0 in enumerate(idx_mb_s0):\n",
    "                _ = c_model.train()\n",
    "\n",
    "                tmp_source0_train = c_model(\n",
    "                    XS0_P_TR[ii_s0, :], XS0_S_TR[ii_s0, :], XS0_T_TR[ii_s0, :]\n",
    "                )\n",
    "                py_source0_train = tmp_source0_train[:, 0:1]\n",
    "                loss_source0_train = criterion_source0(\n",
    "                    py_source0_train, YS0_TR[ii_s0, :]\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_source0_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                _ = c_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                        :, 1:2\n",
    "                    ]\n",
    "                    py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                        :, 2:3\n",
    "                    ]\n",
    "                    loss_source_train = criterion_source(\n",
    "                        py_source_train, YS_TR\n",
    "                    )\n",
    "                    loss_target_train = (\n",
    "                        criterion_target(py_target_train, YT_TR)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_train = alpha1 * loss_source0_train + (\n",
    "                        1.0 - alpha1\n",
    "                    ) * (\n",
    "                        alpha2 * loss_source_train\n",
    "                        + (1.0 - alpha2) * loss_target_train\n",
    "                    )\n",
    "\n",
    "                    py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[:, 1:2]\n",
    "                    loss_target_val = (\n",
    "                        criterion_target(py_target_val, YT_VA)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "\n",
    "                    py_source0_test = c_model(XS0_P_TE, XS0_S_TE, XS0_T_TE)[\n",
    "                        :, 0:1\n",
    "                    ]\n",
    "                    py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[:, 1:2]\n",
    "                    py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)[:, 2:3]\n",
    "                    loss_source0_test = criterion_source0(\n",
    "                        py_source0_test, YS0_TE\n",
    "                    )\n",
    "                    loss_source_test = criterion_source(py_source_test, YS_TE)\n",
    "                    loss_target_test = (\n",
    "                        criterion_target(py_target_test, YT_TE)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_test = alpha1 * loss_source0_test + (1.0 - alpha1) * (\n",
    "                        alpha2 * loss_source_test\n",
    "                        + (1.0 - alpha2) * loss_target_test\n",
    "                    )\n",
    "\n",
    "                learning_curve = pd.concat(\n",
    "                    [\n",
    "                        learning_curve,\n",
    "                        pd.Series(\n",
    "                            {\n",
    "                                \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                \"Loss_Training\": loss_train.item(),\n",
    "                                \"Loss_Test\": loss_test.item(),\n",
    "                            },\n",
    "                            name=f\"pre_{t}\",\n",
    "                        )\n",
    "                        .to_frame()\n",
    "                        .T,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        # main training\n",
    "        best_loss_val = np.inf\n",
    "        for t in range(epochs):\n",
    "            # mini-batch of training data\n",
    "            kf = KFold(n_splits=n_minibatch_PI, shuffle=True)\n",
    "            idx_mb_s0 = [x for _, x in kf.split(XS0_P_TR)]\n",
    "            idx_mb_s, idx_mb_t = [], []\n",
    "            for k in range(n_factor_COSMO):\n",
    "                kf = KFold(n_splits=n_minibatch_COSMO, shuffle=True)\n",
    "                idx_mb_s += [x for _, x in kf.split(XS_P_TR)]\n",
    "            for k in range(n_factor_CHI):\n",
    "                kf = KFold(n_splits=n_minibatch_CHI, shuffle=True)\n",
    "                idx_mb_t += [x for _, x in kf.split(XT_P_TR)]\n",
    "\n",
    "            for tt, (ii_s0, ii_s, ii_t) in enumerate(\n",
    "                zip(idx_mb_s0, idx_mb_s, idx_mb_t)\n",
    "            ):\n",
    "                c_model.train()\n",
    "                if alpha1 > 0:\n",
    "                    py_source0_train = c_model(\n",
    "                        XS0_P_TR[ii_s0, :],\n",
    "                        XS0_S_TR[ii_s0, :],\n",
    "                        XS0_T_TR[ii_s0, :],\n",
    "                    )[:, 0:1]\n",
    "                    loss_source0_train = criterion_source0(\n",
    "                        py_source0_train, YS0_TR[ii_s0, :]\n",
    "                    )\n",
    "                else:\n",
    "                    loss_source0_train = torch.zeros(1, device=device)\n",
    "\n",
    "                if (alpha2 > 0) and (alpha1 < 1):\n",
    "                    if no_COSMO_BN:\n",
    "                        c_model.eval()\n",
    "                    else:\n",
    "                        c_model.train()\n",
    "                    py_source_train = c_model(\n",
    "                        XS_P_TR[ii_s, :], XS_S_TR[ii_s, :], XS_T_TR[ii_s, :]\n",
    "                    )[:, 1:2]\n",
    "                    loss_source_train = criterion_source(\n",
    "                        py_source_train, YS_TR[ii_s, :]\n",
    "                    )\n",
    "                else:\n",
    "                    loss_source_train = torch.zeros(1, device=device)\n",
    "\n",
    "                if (alpha1 < 1) and (alpha2 < 1):\n",
    "                    if no_target_BN:\n",
    "                        c_model.eval()\n",
    "                    else:\n",
    "                        c_model.train()\n",
    "                    py_target_train = c_model(\n",
    "                        XT_P_TR[ii_t, :], XT_S_TR[ii_t, :], XT_T_TR[ii_t, :]\n",
    "                    )[:, 2:3]\n",
    "                    loss_target_train = (\n",
    "                        criterion_target(py_target_train, YT_TR[ii_t, :])\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                else:\n",
    "                    loss_target_train = torch.zeros(1, device=device)\n",
    "\n",
    "                loss_train = alpha1 * loss_source0_train + (1.0 - alpha1) * (\n",
    "                    alpha2 * loss_source_train\n",
    "                    + (1.0 - alpha2) * loss_target_train\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                c_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    py_target_val = c_model(XT_P_VA, XT_S_VA, XT_T_VA)[:, 2:3]\n",
    "                    loss_target_val = (\n",
    "                        criterion_target(py_target_val, YT_VA)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "\n",
    "                    py_source0_test = c_model(XS0_P_TE, XS0_S_TE, XS0_T_TE)[\n",
    "                        :, 0:1\n",
    "                    ]\n",
    "                    py_source_test = c_model(XS_P_TE, XS_S_TE, XS_T_TE)[:, 1:2]\n",
    "                    py_target_test = c_model(XT_P_TE, XT_S_TE, XT_T_TE)\n",
    "                    loss_source0_test = criterion_source0(\n",
    "                        py_source0_test, YS0_TE\n",
    "                    )\n",
    "                    loss_source_test = criterion_source(py_source_test, YS_TE)\n",
    "                    loss_target_test = (\n",
    "                        criterion_target(py_target_test[:, 2:3], YT_TE)\n",
    "                        * loss_factor_target\n",
    "                    )\n",
    "                    loss_test = alpha1 * loss_source0_test + (1.0 - alpha1) * (\n",
    "                        alpha2 * loss_source_test\n",
    "                        + (1.0 - alpha2) * loss_target_test\n",
    "                    )\n",
    "\n",
    "                learning_curve = pd.concat(\n",
    "                    [\n",
    "                        learning_curve,\n",
    "                        pd.Series(\n",
    "                            {\n",
    "                                \"Loss_Source0_Training\": loss_source0_train.item(),\n",
    "                                \"Loss_Source0_Test\": loss_source0_test.item(),\n",
    "                                \"Loss_Source_Training\": loss_source_train.item(),\n",
    "                                \"Loss_Source_Test\": loss_source_test.item(),\n",
    "                                \"Loss_Target_Training\": loss_target_train.item(),\n",
    "                                \"Loss_Target_Validation\": loss_target_val.item(),\n",
    "                                \"Loss_Target_Test\": loss_target_test.item(),\n",
    "                                \"Loss_Training\": loss_train.item(),\n",
    "                                \"Loss_Test\": loss_test.item(),\n",
    "                            },\n",
    "                            name=f\"main_{t}_{tt}\",\n",
    "                        )\n",
    "                        .to_frame()\n",
    "                        .T,\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "                if (t > burn_in) and (loss_target_val < best_loss_val):\n",
    "                    save_NN(\n",
    "                        paras_p,\n",
    "                        paras_s,\n",
    "                        dim_out,\n",
    "                        c_model,\n",
    "                        f\"{dir_save}/best_loss_target_val.pt\",\n",
    "                    )\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        py_source0_train = c_model(\n",
    "                            XS0_P_TR, XS0_S_TR, XS0_T_TR\n",
    "                        )[:, 0:1]\n",
    "                        py_source_train = c_model(XS_P_TR, XS_S_TR, XS_T_TR)[\n",
    "                            :, 1:2\n",
    "                        ]\n",
    "                        py_target_train = c_model(XT_P_TR, XT_S_TR, XT_T_TR)[\n",
    "                            :, 2:3\n",
    "                        ]\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s0.loc[idx_split_s0[\"idx_tr\"], :],\n",
    "                            pd.Series(\n",
    "                                py_source0_train.to(\"cpu\")\n",
    "                                .detach()\n",
    "                                .numpy()\n",
    "                                .flatten(),\n",
    "                                index=idx_split_s0[\"idx_tr\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source0_train.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s.loc[idx_split_s[\"idx_tr\"], :],\n",
    "                            pd.Series(\n",
    "                                ys_scaler.inverse_transform(\n",
    "                                    py_source_train.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_s[\"idx_tr\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source_train.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_tr, :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_train.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_tr,\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_target_train.csv\"\n",
    "                    )\n",
    "\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_val, :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_val.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_val,\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(f\"{dir_save}/best_loss_target_val_target_val.csv\")\n",
    "\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s0.loc[idx_split_s0[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                py_source0_test.to(\"cpu\")\n",
    "                                .detach()\n",
    "                                .numpy()\n",
    "                                .flatten(),\n",
    "                                index=idx_split_s0[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source0_test.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_s.loc[idx_split_s[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                ys_scaler.inverse_transform(\n",
    "                                    py_source_test.to(\"cpu\").detach().numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_s[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_source_test.csv\"\n",
    "                    )\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            y_t.loc[idx_split_t[\"idx_te\"], :],\n",
    "                            pd.Series(\n",
    "                                yt_scaler.inverse_transform(\n",
    "                                    py_target_test[:, 2:3]\n",
    "                                    .to(\"cpu\")\n",
    "                                    .detach()\n",
    "                                    .numpy()\n",
    "                                ).flatten(),\n",
    "                                index=idx_split_t[\"idx_te\"],\n",
    "                                name=\"pred\",\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    ).to_csv(\n",
    "                        f\"{dir_save}/best_loss_target_val_target_test.csv\"\n",
    "                    )\n",
    "\n",
    "                    tmp_mat = py_target_test.to(\"cpu\").detach().numpy()\n",
    "                    tmp_mat[:, 1:2] = ys_scaler.inverse_transform(\n",
    "                        tmp_mat[:, 1:2]\n",
    "                    )\n",
    "                    tmp_mat[:, 2:3] = yt_scaler.inverse_transform(\n",
    "                        tmp_mat[:, 2:3]\n",
    "                    )\n",
    "                    tmp_mat = pd.DataFrame(tmp_mat)\n",
    "                    tmp_mat.columns = [\n",
    "                        \"Soluble\",\n",
    "                        \"Chi_COSMO\",\n",
    "                        \"Chi_Exp\",\n",
    "                        \"Z_sol\",\n",
    "                        \"A_COSMO\",\n",
    "                        \"B_COSMO\",\n",
    "                        \"A_Exp\",\n",
    "                        \"B_Exp\",\n",
    "                    ] + [f\"Z_{x}\" for x in range(dim_out)]\n",
    "                    tmp_mat.to_csv(f\"{dir_save}/output_target_test.csv\")\n",
    "\n",
    "                    best_loss_val = loss_target_val\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        learning_curve.to_csv(f\"{dir_save}/learning_curve.csv\")\n",
    "\n",
    "    print(f\"Finished model {iCV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978922cc-fd8b-4efe-88f4-fb11566e51c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
